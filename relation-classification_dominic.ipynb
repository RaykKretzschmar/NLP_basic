{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, SpanGroup\n",
    "from spacy.matcher import Matcher\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "from tqdm import autonotebook as tqdm\n",
    "from spacy.training import biluo_tags_to_spans\n",
    "import iobes\n",
    "import re\n",
    "from itertools import combinations\n",
    "from spacy import displacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"./data/teaching-dataset\")\n",
    "with (data_dir / \"relation_classification_text_train.zip\").open(\"rb\") as file:\n",
    "    zip_file = ZipFile(file)\n",
    "    with zip_file.open(\"input.txt\") as f:\n",
    "        sentences = [\n",
    "            sentence.split(\"\\n\") for sentence in f.read().decode(\"utf-8\").split(\"\\n\\n\")\n",
    "        ]\n",
    "with (data_dir / \"relation_classification_references_train.zip\").open(\"rb\") as file:\n",
    "    zip_file = ZipFile(file)\n",
    "    with zip_file.open(\"references.txt\") as f:\n",
    "        labels = []\n",
    "        for line in f.read().decode(\"utf-8\").split(\"\\n\"):\n",
    "            relations = []\n",
    "            for relation in re.finditer(r\"\\(\\((\\d+),(\\d+)\\),\\((\\d+),(\\d+)\\)\\)\", line):\n",
    "                relation = (\n",
    "                    (int(relation.group(1)), int(relation.group(2))),\n",
    "                    (int(relation.group(3)), int(relation.group(4))),\n",
    "                )\n",
    "                relations.append(relation)\n",
    "            labels.append(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence(sentence):\n",
    "    words = []\n",
    "    tags = []\n",
    "    for item in sentence:\n",
    "        word, tag = item.split(\" \")\n",
    "        words.append(word)\n",
    "        tags.append(tag)\n",
    "    doc = Doc(nlp.vocab, words=words)\n",
    "    doc = nlp(doc)\n",
    "    tags = iobes.bio_to_bilou(tags)\n",
    "    doc.ents = biluo_tags_to_spans(doc, tags)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According O\n",
      "two O\n",
      "different O\n",
      "studies O\n",
      "it O\n",
      "seems O\n",
      "plausible O\n",
      "that O\n",
      "the O\n",
      "Pohang B-EVENT\n",
      "earthquake I-EVENT\n",
      "was O\n",
      "induced O\n",
      "by O\n",
      "EGS B-EVENT\n",
      "operations I-EVENT\n",
      ". O\n"
     ]
    }
   ],
   "source": [
    "for word in sentences[0]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((14, 16), (9, 11))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Pohang earthquake, EGS operations)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = parse_sentence(sentences[0])\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_active_passive(doc):\n",
    "    # https://stackoverflow.com/questions/74528441/detect-passive-or-active-sentence-from-text\n",
    "    passive_rules = [\n",
    "        [\n",
    "            {\"DEP\": \"nsubjpass\"},\n",
    "            {\"DEP\": \"aux\", \"OP\": \"*\"},\n",
    "            {\"DEP\": \"auxpass\"},\n",
    "            {\"TAG\": \"VBN\"},\n",
    "        ],\n",
    "        [\n",
    "            {\"DEP\": \"nsubjpass\"},\n",
    "            {\"DEP\": \"aux\", \"OP\": \"*\"},\n",
    "            {\"DEP\": \"auxpass\"},\n",
    "            {\"TAG\": \"VBZ\"},\n",
    "        ],\n",
    "        [\n",
    "            {\"DEP\": \"nsubjpass\"},\n",
    "            {\"DEP\": \"aux\", \"OP\": \"*\"},\n",
    "            {\"DEP\": \"auxpass\"},\n",
    "            {\"TAG\": \"RB\"},\n",
    "            {\"TAG\": \"VBN\"},\n",
    "        ],\n",
    "    ]\n",
    "    # Create pattern to match active voice use\n",
    "    active_rules = [\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"VBD\", \"DEP\": \"ROOT\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"VBP\"}, {\"TAG\": \"VBG\", \"OP\": \"!\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"DEP\": \"aux\", \"OP\": \"*\"}, {\"TAG\": \"VB\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"DEP\": \"aux\", \"OP\": \"*\"}, {\"TAG\": \"VBG\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"RB\", \"OP\": \"*\"}, {\"TAG\": \"VBG\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"RB\", \"OP\": \"*\"}, {\"TAG\": \"VBZ\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"RB\", \"OP\": \"+\"}, {\"TAG\": \"VBD\"}],\n",
    "    ]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Passive\", passive_rules)\n",
    "    matcher.add(\"Active\", active_rules)\n",
    "    matches = matcher(doc)\n",
    "    matches = [\n",
    "        (nlp.vocab.strings[match_id], doc[start:end])\n",
    "        for match_id, start, end in matches\n",
    "    ]\n",
    "    return matches\n",
    "\n",
    "\n",
    "def predict(sentence):\n",
    "    doc = parse_sentence(sentence)\n",
    "    matches = extract_active_passive(doc)\n",
    "    predictions = []\n",
    "    for ent_1, ent_2 in combinations(doc.ents, 2):\n",
    "        for match_type, match_span in matches:\n",
    "            if SpanGroup(doc, spans=[ent_1, ent_2, match_span]).has_overlap:\n",
    "                match_active = match_type == \"Active\"\n",
    "                if match_active:\n",
    "                    predictions.append(\n",
    "                        ((ent_1.start, ent_1.end), (ent_2.start, ent_2.end))\n",
    "                    )\n",
    "                    break\n",
    "                else:\n",
    "                    predictions.append(\n",
    "                        ((ent_2.start, ent_2.end), (ent_1.start, ent_1.end))\n",
    "                    )\n",
    "                    break\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According two different studies it seems plausible that the Pohang earthquake was induced by EGS operations . \n",
      "\tActive: it seems\n",
      "\tPassive: earthquake was induced\n"
     ]
    }
   ],
   "source": [
    "doc = parse_sentence(sentences[0])\n",
    "matches = extract_active_passive(doc)\n",
    "print(doc)\n",
    "for match_type, match_span in matches:\n",
    "    print(\"\\t{}: {}\".format(match_type, match_span.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According two different studies it seems plausible that the Pohang earthquake was induced by EGS operations . \n",
      "Ground truth:\n",
      "\tEGS operations -> Pohang earthquake\n",
      "Predictions:\n",
      "\tEGS operations -> Pohang earthquake\n",
      "\n",
      "\n",
      "Signs and symptoms include : Dyspnea ( shortness of breath ) exacerbated by exertion Cough , often persistent and sometimes severe Fatigue Tachypnea ( rapid breathing ) which is often labored , Loss of appetite and weight loss Chest pain Fever Gradual darkening of skin ( blue skin ) Gradual dark shallow rifts in nails eventually leading to cracks as protein fibers within nail beds are destroyed . \n",
      "Ground truth:\n",
      "\tGradual dark shallow rifts in nails -> cracks\n",
      "Predictions:\n",
      "\tGradual dark shallow rifts in nails -> cracks\n",
      "\n",
      "\n",
      "Pneumatic drilling in mines and less commonly , mining using explosives , would raise fine - ultra fine crystalline silica dust ( rock dust ) . \n",
      "Ground truth:\n",
      "\tPneumatic drilling in mines -> fine - ultra fine crystalline silica dust\n",
      "\tmining using explosives -> fine - ultra fine crystalline silica dust\n",
      "Predictions:\n",
      "\tPneumatic drilling in mines -> fine - ultra fine crystalline silica dust\n",
      "\tmining using explosives -> fine - ultra fine crystalline silica dust\n",
      "\n",
      "\n",
      "Interleukin-1 inhibitors , such as canakinumab , showed moderate effectiveness for pain relief and reduction of joint swelling , but have increased risk of adverse events , such as back pain , headache , and increased blood pressure . \n",
      "Ground truth:\n",
      "\tcanakinumab -> pain relief\n",
      "\tcanakinumab -> reduction of joint swelling\n",
      "\tcanakinumab -> adverse events\n",
      "\tcanakinumab -> back pain\n",
      "\tcanakinumab -> headache\n",
      "\tcanakinumab -> increased blood pressure\n",
      "\tInterleukin-1 inhibitors -> pain relief\n",
      "\tInterleukin-1 inhibitors -> reduction of joint swelling\n",
      "\tInterleukin-1 inhibitors -> adverse events\n",
      "\tInterleukin-1 inhibitors -> back pain\n",
      "\tInterleukin-1 inhibitors -> headache\n",
      "\tInterleukin-1 inhibitors -> increased blood pressure\n",
      "Predictions:\n",
      "\tInterleukin-1 inhibitors -> pain relief\n",
      "\tInterleukin-1 inhibitors -> reduction of joint swelling\n",
      "\tInterleukin-1 inhibitors -> adverse events\n",
      "\tInterleukin-1 inhibitors -> adverse events\n",
      "\tInterleukin-1 inhibitors -> back pain\n",
      "\tInterleukin-1 inhibitors -> back pain\n",
      "\tInterleukin-1 inhibitors -> headache\n",
      "\tInterleukin-1 inhibitors -> headache\n",
      "\tInterleukin-1 inhibitors -> increased blood pressure\n",
      "\tInterleukin-1 inhibitors -> increased blood pressure\n",
      "\tcanakinumab -> pain relief\n",
      "\tcanakinumab -> reduction of joint swelling\n",
      "\tcanakinumab -> adverse events\n",
      "\tcanakinumab -> adverse events\n",
      "\tcanakinumab -> back pain\n",
      "\tcanakinumab -> back pain\n",
      "\tcanakinumab -> headache\n",
      "\tcanakinumab -> headache\n",
      "\tcanakinumab -> increased blood pressure\n",
      "\tcanakinumab -> increased blood pressure\n",
      "\tpain relief -> adverse events\n",
      "\tpain relief -> back pain\n",
      "\tpain relief -> headache\n",
      "\tpain relief -> increased blood pressure\n",
      "\treduction of joint swelling -> adverse events\n",
      "\treduction of joint swelling -> back pain\n",
      "\treduction of joint swelling -> headache\n",
      "\treduction of joint swelling -> increased blood pressure\n",
      "\n",
      "\n",
      "Two of the reviews postulate that exercise is not essential for the development of symptoms , but rather that it is one of several augmentation factors , citing evidence that the culprit food in combination with alcohol or aspirin will result in a respiratory anaphylactic reaction . \n",
      "Ground truth:\n",
      "\tfood in combination with alcohol or aspirin -> respiratory anaphylactic reaction\n",
      "Predictions:\n",
      "\tfood in combination with alcohol or aspirin -> respiratory anaphylactic reaction\n",
      "\n",
      "\n",
      "Serum sickness can be developed as a result of exposure to antibodies derived from animals . \n",
      "Ground truth:\n",
      "\tSerum sickness -> exposure to antibodies derived from animals\n",
      "Predictions:\n",
      "\tSerum sickness -> exposure to antibodies derived from animals\n",
      "\n",
      "\n",
      "Dynapenia ( pronounced dahy - nuh - pē - nē- a , Greek translation for poverty of strength , power , or force ) is the loss of muscular strength not caused by neurological or muscular disease that typically is associated with older adults . \n",
      "Ground truth:\n",
      "\tneurological or muscular disease -> Dynapenia\n",
      "Predictions:\n",
      "\tDynapenia -> neurological or muscular disease\n",
      "\tneurological or muscular disease -> Dynapenia\n",
      "\n",
      "\n",
      "Inflamed facets can cause a powerful muscle spasm . \n",
      "Ground truth:\n",
      "\tInflamed facets -> powerful muscle spasm\n",
      "Predictions:\n",
      "\tInflamed facets -> powerful muscle spasm\n",
      "\n",
      "\n",
      "Large hemangiomas can leave visible skin changes secondary to severe stretching that results in altered surface texture . \n",
      "Ground truth:\n",
      "\tLarge hemangiomas -> visible skin changes\n",
      "\tLarge hemangiomas -> altered surface texture\n",
      "Predictions:\n",
      "\tLarge hemangiomas -> visible skin changes\n",
      "\tLarge hemangiomas -> altered surface texture\n",
      "\tLarge hemangiomas -> altered surface texture\n",
      "\tvisible skin changes -> altered surface texture\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def in_between(token, outer1, outer2):\n",
    "    return  token.i <= outer1.start and token.i >= outer2.end or \\\n",
    "            token.i <= outer2.start and token.i >= outer1.end\n",
    "\n",
    "\n",
    "def predict_2(sentence):\n",
    "    doc = parse_sentence(sentence)\n",
    "    events = doc.ents\n",
    "    predictions = []\n",
    "\n",
    "    # find verbs\n",
    "    verb_pattern = [[{'POS': 'VERB'},]]\n",
    "    verb_matcher = Matcher(nlp.vocab)\n",
    "    verb_matcher.add(\"verbs\", verb_pattern)\n",
    "    matches = verb_matcher(doc)\n",
    "    verbs = [(doc[start:end]) for _, start, end in matches]\n",
    "    verbs = [doc[verb.start] for verb in verbs] # get tokens instead of spans\n",
    "\n",
    "    # remove verbs inside events\n",
    "    for verb in verbs:\n",
    "        for ent in events:\n",
    "            if verb.i >= ent.start and verb.i <= ent.end:\n",
    "                verbs.remove(verb)\n",
    "    # remove verbs inside brackets\n",
    "    \n",
    "\n",
    "    for ent_1, ent_2 in combinations(doc.ents, 2):\n",
    "        # entities are actually in order\n",
    "        for verb in verbs:\n",
    "            if in_between(verb, ent_1, ent_2):\n",
    "                backwards = False\n",
    "                for child in verb.children:\n",
    "                    if child.dep_ in [\"agent\"]:\n",
    "                        backwards = True\n",
    "                if backwards:\n",
    "                    predictions.append(((ent_2.start, ent_2.end), (ent_1.start, ent_1.end)))\n",
    "                else:\n",
    "                    predictions.append(((ent_1.start, ent_1.end), (ent_2.start, ent_2.end)))\n",
    "\n",
    "    return predictions\n",
    "    \n",
    "\n",
    "\n",
    "for idx in [0,1,2,3,4,5,6,7,9]:\n",
    "    doc = parse_sentence(sentences[idx])\n",
    "    print(doc)\n",
    "    pred = predict_2(sentences[idx])\n",
    "\n",
    "    print(\"Ground truth:\")\n",
    "    for cause, effect in labels[idx]:\n",
    "        print(\"\\t{} -> {}\".format(doc[cause[0]:cause[1]], doc[effect[0]:effect[1]]))\n",
    "    print(\"Predictions:\")\n",
    "    for cause, effect in pred:\n",
    "        print(\"\\t{} -> {}\".format(doc[cause[0]:cause[1]], doc[effect[0]:effect[1]]))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# displacy.render(doc, style=\"dep\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adposition'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"ADP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0944c4da9346508b5122ec5cbe8174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = []\n",
    "for sentence in tqdm.tqdm(sentences):\n",
    "    predictions.append(predict_2(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Precision: 0.52\n",
      "Micro Recall: 0.63\n",
      "Micro F1: 0.57\n",
      "Macro Precision: 0.56\n",
      "Macro Recall: 0.63\n",
      "Macro F1: 0.58\n"
     ]
    }
   ],
   "source": [
    "def evaluate(predictions, references, micro_avg=True):\n",
    "    tp = []\n",
    "    fp = []\n",
    "    fn = []\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        tp.append(len(set(prediction) & set(reference)))\n",
    "        fp.append(len(set(prediction) - set(reference)))\n",
    "        fn.append(len(set(reference) - set(prediction)))\n",
    "    if micro_avg:\n",
    "        tp = [sum(tp)]\n",
    "        fp = [sum(fp)]\n",
    "        fn = [sum(fn)]\n",
    "    precision = [0 if tp[i] == 0 else tp[i] / (tp[i] + fp[i]) for i in range(len(tp))]\n",
    "    recall = [0 if tp[i] == 0 else tp[i] / (tp[i] + fn[i]) for i in range(len(tp))]\n",
    "    f1 = [\n",
    "        0\n",
    "        if precision[i] * recall[i] == 0\n",
    "        else 2 * precision[i] * recall[i] / (precision[i] + recall[i])\n",
    "        for i in range(len(tp))\n",
    "    ]\n",
    "    precision = sum(precision) / len(precision)\n",
    "    recall = sum(recall) / len(recall)\n",
    "    f1 = sum(f1) / len(f1)\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "micro_precision, micro_recall, micro_f1 = evaluate(predictions, labels, True)\n",
    "macro_precision, macro_recall, macro_f1 = evaluate(predictions, labels, False)\n",
    "\n",
    "print(\"Micro Precision: {:.2f}\".format(micro_precision))\n",
    "print(\"Micro Recall: {:.2f}\".format(micro_recall))\n",
    "print(\"Micro F1: {:.2f}\".format(micro_f1))\n",
    "print(\"Macro Precision: {:.2f}\".format(macro_precision))\n",
    "print(\"Macro Recall: {:.2f}\".format(macro_recall))\n",
    "print(\"Macro F1: {:.2f}\".format(macro_f1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
