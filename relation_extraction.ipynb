{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "from tqdm import autonotebook as tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy.tokens import Doc\n",
    "import re\n",
    "from collections import Counter\n",
    "from collections.abc import Iterable\n",
    "from spacy.training import biluo_tags_to_spans\n",
    "from itertools import combinations\n",
    "import iobes\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "from seqeval import scheme\n",
    "from tqdm import autonotebook as tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import sklearn_crfsuite\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The longest serving spacecraft goes into retirement . "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "data_dir = Path(\"./data/teaching-dataset\")\n",
    "with (data_dir / \"relation_extraction_text_train.zip\").open(\"rb\") as file:\n",
    "    zip_file = ZipFile(file)\n",
    "    with zip_file.open(\"input.txt\") as f:\n",
    "        sentences = [\n",
    "            sentence.split(\"\\n\") for sentence in f.read().decode(\"utf-8\").split(\"\\n\\n\")\n",
    "        ]\n",
    "with (data_dir / \"relation_extraction_references_train.zip\").open(\"rb\") as file:\n",
    "    zip_file = ZipFile(file)\n",
    "    with zip_file.open(\"references.txt\") as f:\n",
    "        labels = []\n",
    "        for line in f.read().decode(\"utf-8\").split(\"\\n\"):\n",
    "            relations = []\n",
    "            for relation in re.finditer(r\"\\(\\((\\d+),(\\d+)\\),\\((\\d+),(\\d+)\\)\\)\", line):\n",
    "                relation = (\n",
    "                    (int(relation.group(1)), int(relation.group(2))),\n",
    "                    (int(relation.group(3)), int(relation.group(4))),\n",
    "                )\n",
    "                relations.append(relation)\n",
    "            labels.append(relations)\n",
    "assert len(sentences) == len(labels)\n",
    "doc = nlp(Doc(nlp.vocab, words=sentences[0]))\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "train_sentences, val_sentences, train_labels, val_labels =  train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
    "print(len(train_sentences))\n",
    "print(len(val_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class is_causal_predictor:\n",
    "    def __init__(self, sentences, labels, n=None):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.n = n\n",
    "        \n",
    "        if n != None:\n",
    "            self.init_words(sentences, labels)\n",
    "            self.causal_cues = self.get_causal_cues(self.n)\n",
    "        else:\n",
    "            self.init_causal_cues_best_n(sentences, labels)\n",
    "\n",
    "\n",
    "    def init_words(self, sentences, labels):\n",
    "        self.words = []\n",
    "        self.nonCausalWords = []\n",
    "\n",
    "        for label, sentence in zip(labels, sentences):\n",
    "            if type(sentence) == list:\n",
    "                sentence = ' '.join(sentence)\n",
    "            if label != []: #if sentence is causal\n",
    "                doc = nlp(sentence)\n",
    "                wordsHelp = [token.text for token in doc if not token.is_stop and not token.is_punct and token.pos_ != \"NOUN\" and token.pos_ != \"ADJ\"] \n",
    "                self.words.extend(wordsHelp) #append all words to a list if they are NOT nouns & NOT punctuation & NOT adjectives\n",
    "\n",
    "            else: #if sentence is NOT causal\n",
    "                doc = nlp(sentence)\n",
    "                wordsHelp = [token.text for token in doc if not token.is_stop and not token.is_punct and token.pos_ != \"NOUN\" and token.pos_ != \"ADJ\"] \n",
    "                self.nonCausalWords.extend(wordsHelp)  \n",
    "    \n",
    "\n",
    "    # returns n best causal cues\n",
    "    def get_causal_cues(self, n):\n",
    "        def flatten(lis): #pretty ugly solution but we have to flatten the list since every new sentence adds \"[]\" which Counter can't deal with\n",
    "            for item in lis:\n",
    "                if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "                    for x in flatten(item):\n",
    "                        yield x\n",
    "                else:        \n",
    "                    yield item\n",
    "        \n",
    "        def get_n_lemmata(causal_freq, n):\n",
    "            # sort words\n",
    "            sorted_words = np.array(causal_freq.most_common(len(causal_freq)))[:,0]\n",
    "            converted_return = []\n",
    "            for word in sorted_words:\n",
    "                # lemmatize\n",
    "                doc = nlp(str(word))\n",
    "                word = \" \".join([token.lemma_ for token in doc])\n",
    "                if not word in converted_return:\n",
    "                    converted_return.append(word)\n",
    "                # break if n lemmata found\n",
    "                if len(converted_return)==n:\n",
    "                    break\n",
    "            return converted_return\n",
    "\n",
    "        causal_freq = Counter(self.words)\n",
    "        nonCausal_freq = Counter(self.nonCausalWords)\n",
    "        for word in causal_freq:\n",
    "            causal_freq[word] = causal_freq[word]/(nonCausal_freq[word]+1)\n",
    "        return get_n_lemmata(causal_freq, n)\n",
    "\n",
    "\n",
    "    def predict_causality(self, sentence):\n",
    "        if type(sentence) == list: # convert to str\n",
    "            sentence = ' '.join(sentence)\n",
    "        matcher = Matcher(nlp.vocab)\n",
    "        pattern = [[{\"LEMMA\": cue}] for cue in self.causal_cues]\n",
    "        matcher.add(\"CAUSAL\", pattern)\n",
    "        doc = nlp(sentence)\n",
    "        matches = matcher(doc)\n",
    "        return bool(matches)\n",
    "\n",
    "\n",
    "    # Find best value for n given a testset and initialize causal cues according to best n\n",
    "    def init_causal_cues_best_n(self, sentences, labels, step_size=5):\n",
    "        train_sentences, test_sentences, train_labels, test_labels =  train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
    "        self.init_words(train_sentences, train_labels)\n",
    "\n",
    "        new_labels = []\n",
    "        for label in test_labels:\n",
    "            if label != []:\n",
    "                new_labels.append(1)\n",
    "            else:\n",
    "                new_labels.append(0)\n",
    "\n",
    "        old_f1 = 0\n",
    "        f1 = 0\n",
    "        n = 0\n",
    "        while f1 >= old_f1:\n",
    "            n = n+step_size\n",
    "            predictions = []\n",
    "\n",
    "            # predict\n",
    "            self.causal_cues = self.get_causal_cues(n)\n",
    "            for sentence, l in zip(test_sentences, test_labels):\n",
    "                p = self.predict_causality(sentence)\n",
    "                predictions.append(p)\n",
    "\n",
    "            # evaluate\n",
    "            old_f1 = f1\n",
    "            tp = sum([int(p) == 1 and int(l) == 1 for p, l in zip(predictions, new_labels)])\n",
    "            fp = sum([int(p) == 1 and int(l) == 0 for p, l in zip(predictions, new_labels)])\n",
    "            fn = sum([int(p) == 0 and int(l) == 1 for p, l in zip(predictions, new_labels)])\n",
    "            precision = tp / (tp + fp)\n",
    "            recall = tp / (tp + fn)\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "            x = 0\n",
    "\n",
    "        self.f1 = old_f1 \n",
    "        self.n = n - step_size # go back to best value for n\n",
    "        self.causal_cues = self.get_causal_cues(self.n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetterTrain(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, d, t) for w, p, d, t in zip(s['Word'].values.tolist(), \n",
    "                                                           s['POS'].values.tolist(), \n",
    "                                                           s['DEP'].values.tolist(),\n",
    "                                                           s['Tag'].values.tolist())]\n",
    "        self.grouped = self.data.groupby('Sentence #').apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    def get_next(self):\n",
    "        try: \n",
    "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s \n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    deptag = sent[i][2]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0, \n",
    "        'word.lower()': word.lower(), \n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "        'deptag': deptag,\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        deptag1 = sent[i-1][2]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "            '-1:deptag': deptag1,\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        # print(i)\n",
    "        deptag1 = sent[i+1][2]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "            '+1:deptag': deptag1,\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, deptag, label in sent]\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, deptag, label in sent]\n",
    "\n",
    "def flatten(l):\n",
    "    output = []\n",
    "    for sublist in l:\n",
    "        for item in sublist:\n",
    "            output.append(item)\n",
    "\n",
    "        output.append('')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training ML-model with data from exercise 2\n",
    "def train_step_2():\n",
    "    data_dir = Path(\"./data/teaching-dataset\")\n",
    "    with (data_dir / \"span_extraction_text_train.zip\").open(\"rb\") as file:\n",
    "        zip_file = ZipFile(file)\n",
    "        with zip_file.open(\"input.txt\") as f:\n",
    "            sentences = [\n",
    "                sentence.split(\"\\n\") for sentence in f.read().decode(\"utf-8\").split(\"\\n\\n\")\n",
    "            ]\n",
    "    with (data_dir / \"span_extraction_references_train.zip\").open(\"rb\") as file:\n",
    "        zip_file = ZipFile(file)\n",
    "        with zip_file.open(\"references.txt\") as f:\n",
    "            labels = [\n",
    "                sentence.split(\"\\n\") for sentence in f.read().decode(\"utf-8\").split(\"\\n\\n\")\n",
    "            ]\n",
    "\n",
    "    sentence_number=[]\n",
    "    words=[]\n",
    "    tags=[]\n",
    "    dependencies=[]\n",
    "    events=[]\n",
    "    for i in range(len(sentences)):\n",
    "        doc = Doc(nlp.vocab, words=sentences[i])\n",
    "        doc = nlp(doc)\n",
    "        iterator = zip(doc, labels[i])\n",
    "        for token, label in iterator:\n",
    "            sentence_number.append(f'Sentence: {i}')\n",
    "            words.append(str(token))\n",
    "            tags.append(token.pos_)\n",
    "            dependencies.append(token.dep_)\n",
    "            events.append(label)\n",
    "\n",
    "    train_data = {'Sentence #': sentence_number,'Word': words, 'POS': tags, 'DEP': dependencies, 'Tag': events}\n",
    "    df = pd.DataFrame(data=train_data)\n",
    "    # print(df)\n",
    "\n",
    "    df.isnull().sum()\n",
    "\n",
    "    df = df.fillna(method='ffill')\n",
    "    df['Sentence #'].nunique(), df.Word.nunique(), df.Tag.nunique();\n",
    "\n",
    "    df.groupby('Tag').size().reset_index(name='counts');\n",
    "\n",
    "    X = df.drop('Tag', axis=1)\n",
    "    v = DictVectorizer(sparse=False)\n",
    "    X = v.fit_transform(X.to_dict('records'))\n",
    "    y = df.Tag.values\n",
    "    classes = np.unique(y)\n",
    "    classes = classes.tolist()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n",
    "    X_train.shape, y_train.shape;\n",
    "\n",
    "    getter = SentenceGetterTrain(df)\n",
    "    sentences = getter.sentences\n",
    "\n",
    "    X = [sent2features(s) for s in sentences]\n",
    "    y = [sent2labels(s) for s in sentences]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=0)\n",
    "\n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "        algorithm='lbfgs',\n",
    "        c1=0.1,\n",
    "        c2=0.1,\n",
    "        max_iterations=100,\n",
    "        all_possible_transitions=True\n",
    "    )\n",
    "    crf.fit(X_train, y_train);\n",
    "\n",
    "    y_pred = crf.predict(X_test)\n",
    "    # print(metrics.flat_classification_report(y_pred=y_pred, y_true=y_test, labels = new_classes))\n",
    "    # sollte eigentlich funktionieren tuts aber nicht\n",
    "    flat_y_true = flatten(y_test)\n",
    "    flat_y_pred = flatten(y_pred)\n",
    "    # print(classification_report(flat_y_true, flat_y_pred))\n",
    "    return(crf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, d) for w, p, d in zip(s['Word'].values.tolist(), \n",
    "                                                           s['POS'].values.tolist(), \n",
    "                                                           s['DEP'].values.tolist())]\n",
    "        self.grouped = self.data.groupby('Sentence #').apply(agg_func)\n",
    "        \n",
    "\n",
    "        # print(self.grouped.head(20))\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "        \n",
    "    def get_next(self):\n",
    "        try: \n",
    "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s \n",
    "        except:\n",
    "            return None\n",
    "\n",
    "class span_extractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def create_dataframe(self, test_sentences):\n",
    "        sentence_number=[]\n",
    "        words=[]\n",
    "        tags=[]\n",
    "        dependencies=[]\n",
    "        events=[]\n",
    "        for i in range(len(test_sentences)):\n",
    "            doc = Doc(nlp.vocab, words=test_sentences[i])\n",
    "            doc = nlp(doc)\n",
    "            for token in doc:\n",
    "                sentence_number.append(f'Sentence: {i}')\n",
    "                words.append(str(token))\n",
    "                tags.append(token.pos_)\n",
    "                dependencies.append(token.dep_)\n",
    "\n",
    "        train_data = {'Sentence #': sentence_number,'Word': words, 'POS': tags, 'DEP': dependencies}\n",
    "        df = pd.DataFrame(data=train_data)\n",
    "        return df\n",
    "\n",
    "    def getSentences(self, df):\n",
    "        getter = SentenceGetter(df)\n",
    "        sentences = getter.sentences\n",
    "        return sentences\n",
    "        # for sentence in sentences:\n",
    "        #     print(sentence)\n",
    "\n",
    "    def flatten(l):\n",
    "        output = []\n",
    "        for sublist in l:\n",
    "            for item in sublist:\n",
    "                output.append(item)\n",
    "\n",
    "            output.append('')\n",
    "        return output\n",
    "    \n",
    "    def cleanup(self, prediction):\n",
    "        counter = 0\n",
    "        for e in prediction:\n",
    "            if e == 'B-EVENT':\n",
    "                counter += 1\n",
    "\n",
    "        if counter < 2:\n",
    "            prediction = False\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def predict(self, sentence, crf):\n",
    "        df = self.create_dataframe(sentence)\n",
    "        sentences = self.getSentences(df)\n",
    "        \n",
    "        X = [sent2features(s) for s in sentences]\n",
    "        y_pred = crf.predict(X)\n",
    "        flat_y_pred = flatten(y_pred)\n",
    "        prediction = self.cleanup(flat_y_pred)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relation_classificator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # [HELPER] adds event spans to doc\n",
    "    def parse_sentence(self, sentence, spans):\n",
    "        words = []\n",
    "        tags = []\n",
    "        for word, tag in zip(sentence, spans):\n",
    "            words.append(word)\n",
    "            tags.append(tag)\n",
    "        doc = Doc(nlp.vocab, words=words)\n",
    "        doc = nlp(doc)\n",
    "        tags = iobes.bio_to_bilou(tags)\n",
    "        doc.ents = biluo_tags_to_spans(doc, tags)\n",
    "        return doc\n",
    "\n",
    "    # [HELPER] is the token in between 2 events?\n",
    "    def token_in_between_events(self, token, outer1, outer2):\n",
    "        return  token.i <= outer1.start and token.i >= outer2.end or \\\n",
    "                token.i <= outer2.start and token.i >= outer1.end\n",
    "\n",
    "    # [HELPER] is the token inside an event?\n",
    "    def inside_event(self, event, token):\n",
    "        return event.start <= token.i and event.end >= token.i\n",
    "\n",
    "    # [HELPER] are both events inside a token's subtree?\n",
    "    def events_inside_subtree(self, verb, event1, event2):\n",
    "        a = False\n",
    "        b = False\n",
    "        for sub in verb.subtree:\n",
    "            if self.inside_event(event1, sub):\n",
    "                a = True\n",
    "        for sub in verb.subtree:\n",
    "            if self.inside_event(event2, sub):\n",
    "                b = True\n",
    "        return a and b\n",
    "\n",
    "    # [HELPER] does the sentence induce a backwards relation?\n",
    "    def backwards(self, verb, doc):\n",
    "        keyword = None\n",
    "        backwards = False\n",
    "        for child in verb.children:\n",
    "            if child.dep_ in [\"agent\"]:\n",
    "                backwards = True\n",
    "                keyword = child\n",
    "            if child.text in [\"from\"]:\n",
    "                backwards = True\n",
    "                keyword = child\n",
    "        next_word = doc[verb.i+1]\n",
    "        if next_word.dep_ in [\"aux\"]:\n",
    "            backwards = True\n",
    "            keyword = next_word\n",
    "        return backwards, keyword\n",
    "\n",
    "    # [HELPER] what's the next event after a position?\n",
    "    def get_next_event(self, doc, position):\n",
    "        lowest_distance = float('inf')\n",
    "        for event in doc.ents:\n",
    "            if abs(event.start - position) <= lowest_distance and position > event.end or position < event.start:\n",
    "                next_event = event\n",
    "                lowest_distance = abs(event.start - position)\n",
    "                return next_event\n",
    "\n",
    "    # [HELPER] make predictions for structures like \"x is cause of/for y\"\n",
    "    def handle_cause_of(self, predictions, doc):\n",
    "        for token in doc:\n",
    "            if token.text == \"cause\" and doc[token.i+1].text in [\"of\", \"for\"]:\n",
    "                for event1, event2 in combinations(doc.ents, 2):\n",
    "                    if self.token_in_between_events(token, event1, event2):\n",
    "                        predictions.append(((event1.start, event1.end), (event2.start, event2.end)))\n",
    "                if len(predictions) == 0:\n",
    "                    effect = self.get_next_event(doc, token.i)\n",
    "                    cause = self.get_next_event(doc, effect.end)\n",
    "                    predictions.append(((cause.start, cause.end), (effect.start, effect.end)))\n",
    "        return predictions\n",
    "\n",
    "    # [HELPER] make predictions for structures like \"x because/due/common_with y\"\n",
    "    def handle_because(self, predictions, doc):\n",
    "        for token in doc:\n",
    "            if token.text == \"because\" or token.text == \"due\" or token.text == \"common\" and doc[token.i+1].text == \"with\":\n",
    "                for event1, event2 in combinations(doc.ents, 2):\n",
    "                    if self.token_in_between_events(token, event1, event2):\n",
    "                        predictions.append(((event2.start, event2.end), (event1.start, event1.end)))\n",
    "                if len(predictions) == 0:\n",
    "                    effect = self.get_next_event(doc, token.i)\n",
    "                    cause = self.get_next_event(doc, effect.end)\n",
    "                    predictions.append(((cause.start, cause.end), (effect.start, effect.end)))\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    # [PREDICTOR] method to make causal relation predictions from given sentence and span string\n",
    "    def predict(self, sentence, spans):\n",
    "        doc = self.parse_sentence(sentence, spans)\n",
    "        events = doc.ents\n",
    "        predictions = []\n",
    "\n",
    "        predictions = self.handle_cause_of(predictions, doc)\n",
    "        predictions = self.handle_because(predictions, doc)\n",
    "\n",
    "        # find verbs\n",
    "        verb_pattern = [[{'POS': 'VERB'}]]\n",
    "        verb_matcher = Matcher(nlp.vocab)\n",
    "        verb_matcher.add(\"verbs\", verb_pattern)\n",
    "        matches = verb_matcher(doc)\n",
    "        verbs = [(doc[start:end]) for _, start, end in matches]\n",
    "        verbs = [doc[verb.start] for verb in verbs] # get tokens instead of spans\n",
    "\n",
    "        # remove verbs inside events\n",
    "        for verb in verbs:\n",
    "            for event in events:\n",
    "                if verb.i >= event.start and verb.i < event.end:\n",
    "                    verbs.remove(verb)\n",
    "\n",
    "        for event1, event2 in combinations(doc.ents, 2): # events are actually in order\n",
    "            for verb in verbs:\n",
    "                is_backwards, keyword = self.backwards(verb, doc)\n",
    "                # predict if both events inside the verb's subtree and the verb is in between events\n",
    "                if is_backwards:\n",
    "                    if self.events_inside_subtree(verb, event1, event2) and self.token_in_between_events(keyword, event1, event2):\n",
    "                        predictions.append(((event2.start, event2.end), (event1.start, event1.end)))\n",
    "                else:\n",
    "                    if self.events_inside_subtree(verb, event1, event2) and self.token_in_between_events(verb, event1, event2):\n",
    "                        predictions.append(((event1.start, event1.end), (event2.start, event2.end)))\n",
    "        \n",
    "        # if there is no prediction yet, use a less strict rule\n",
    "        if len(predictions) == 0:\n",
    "            for event1, event2 in combinations(doc.ents, 2):\n",
    "                for verb in verbs:\n",
    "                    is_backwards, keyword = self.backwards(verb, doc) \n",
    "                    # predict if the verb is in between events - less strict\n",
    "                    if is_backwards: \n",
    "                        if self.token_in_between_events(keyword, event1, event2):\n",
    "                            predictions.append(((event2.start, event2.end), (event1.start, event1.end)))\n",
    "                    else:\n",
    "                        if self.token_in_between_events(verb, event1, event2):\n",
    "                            predictions.append(((event1.start, event1.end), (event2.start, event2.end)))\n",
    "\n",
    "        # default predictions if nothing worked: \"everything causes everything\"\n",
    "        if len(predictions) == 0:\n",
    "            for event1, event2 in combinations(doc.ents, 2):\n",
    "                predictions.append(((event1.start, event1.end), (event2.start, event2.end)))\n",
    "\n",
    "        predictions = set(predictions)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pipeline:\n",
    "    def __init__(self, train_sentences, train_labels):\n",
    "        self.train_sentences = train_sentences\n",
    "        self.train_labels = train_labels\n",
    "        \n",
    "        self.is_causal_predictor = is_causal_predictor(train_sentences, train_labels)\n",
    "        self.span_extractor = span_extractor()\n",
    "        self.relation_classificator = relation_classificator()\n",
    "\n",
    "        self.crf = train_step_2()\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        is_causal = self.is_causal_predictor.predict_causality(sentence)\n",
    "        if is_causal:\n",
    "            spans = self.span_extractor.predict([sentence], self.crf)\n",
    "            if spans:\n",
    "                prediction = self.relation_classificator.predict(sentence, spans)\n",
    "            else:\n",
    "                prediction = []\n",
    "        else:\n",
    "            prediction = []\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.predict(sentences[11])\n",
    "val_predictions = []\n",
    "for s in val_sentences:\n",
    "    val_predictions.append(pipe.predict(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], {((2, 3), (7, 10))}, [], [], [], [], [], [], [], [], [], {((1, 2), (24, 29))}, [], [], {((0, 2), (5, 10))}, {((0, 4), (11, 13)), ((0, 4), (26, 27)), ((0, 4), (20, 21)), ((0, 4), (48, 49)), ((0, 4), (32, 34)), ((0, 4), (39, 40))}, [], [], {((4, 6), (11, 19))}, {((21, 33), (13, 17)), ((19, 20), (13, 17))}, [], [], [], {((5, 7), (22, 27)), ((8, 16), (22, 27))}, {((19, 21), (27, 32))}, [], [], {((27, 29), (31, 35)), ((1, 7), (31, 35)), ((1, 7), (27, 29))}, [], [], {((12, 14), (28, 30)), ((12, 14), (15, 16)), ((15, 16), (28, 30))}, [], {((4, 10), (39, 47)), ((4, 10), (16, 17)), ((4, 10), (60, 63)), ((4, 10), (23, 26)), ((4, 10), (48, 52)), ((4, 10), (12, 14)), ((4, 10), (64, 67)), ((4, 10), (18, 19))}, {((7, 11), (35, 36)), ((7, 11), (38, 40)), ((7, 11), (15, 21)), ((7, 11), (22, 23)), ((7, 11), (33, 34)), ((7, 11), (24, 26)), ((7, 11), (27, 30)), ((7, 11), (12, 14))}, [], {((27, 30), (14, 16))}, [], {((0, 2), (14, 17)), ((0, 2), (4, 7))}, [], {((31, 33), (39, 41)), ((34, 36), (39, 41))}, [], {((2, 6), (7, 12))}, [], [], [], [], [], [], [], [], {((7, 13), (17, 20))}, {((8, 11), (14, 21)), ((0, 4), (14, 21))}, [], {((15, 19), (25, 26)), ((15, 19), (22, 23))}, [], {((33, 36), (6, 7)), ((37, 38), (6, 7)), ((33, 36), (4, 5)), ((37, 38), (4, 5))}, {((1, 5), (9, 16))}, [], {((8, 9), (11, 13))}, {((8, 13), (16, 20))}, {((26, 29), (23, 24))}, [], [], [], [], {((2, 3), (6, 15))}, [], [], {((1, 11), (21, 24))}, [], {((7, 10), (5, 6))}, [], [], {((1, 2), (18, 23)), ((4, 13), (28, 30)), ((4, 13), (24, 26)), ((1, 2), (28, 30)), ((1, 2), (24, 26)), ((4, 13), (18, 23))}, [], [], {((1, 4), (9, 14))}, [], [], {((0, 2), (7, 13))}, {((9, 10), (19, 21))}, [], [], [], {((6, 7), (12, 13))}, {((8, 12), (4, 6))}, [], [], [], {((2, 4), (5, 15)), ((2, 4), (18, 19))}, {((2, 6), (15, 23))}, {((0, 2), (4, 8))}, [], {((15, 18), (21, 25)), ((2, 4), (21, 25)), ((2, 4), (15, 18)), ((11, 13), (15, 18)), ((2, 4), (11, 13)), ((11, 13), (21, 25))}, {((1, 9), (14, 21))}, {((12, 20), (23, 34))}, {((0, 1), (27, 28)), ((27, 28), (0, 1)), ((0, 1), (17, 19)), ((0, 1), (7, 9)), ((27, 28), (17, 19)), ((27, 28), (7, 9))}, [], [], [], {((16, 19), (28, 29)), ((7, 10), (28, 29)), ((11, 14), (28, 29))}, {((13, 14), (0, 2)), ((15, 18), (0, 2)), ((20, 21), (0, 2))}, [], {((1, 5), (7, 8))}, {((1, 5), (17, 22)), ((6, 12), (17, 22))}, {((9, 10), (15, 17))}, {((14, 29), (6, 10)), ((35, 43), (6, 10)), ((47, 54), (6, 10))}, {((6, 13), (16, 33))}, [], [], [], {((4, 5), (7, 17)), ((4, 5), (19, 25))}, [], [], {((0, 5), (9, 11))}, [], [], {((22, 29), (16, 20))}, {((1, 5), (8, 12))}, {((0, 1), (3, 5))}, {((54, 59), (48, 50)), ((54, 59), (0, 2))}, {((29, 32), (33, 35))}, [], [], {((4, 6), (13, 14)), ((4, 6), (16, 21)), ((4, 6), (28, 29))}, [], {((11, 12), (15, 16)), ((4, 8), (13, 14)), ((4, 8), (11, 12)), ((13, 14), (15, 16)), ((4, 8), (15, 16)), ((11, 12), (13, 14))}, [], [], {((0, 3), (6, 8)), ((0, 3), (9, 12))}, [], {((11, 14), (7, 9))}, {((3, 5), (16, 20))}, [], [], {((2, 10), (59, 65)), ((22, 24), (59, 65)), ((53, 55), (59, 65))}, [], [], {((17, 18), (1, 2)), ((9, 16), (1, 2)), ((9, 16), (17, 18))}, [], {((11, 13), (39, 42)), ((7, 9), (39, 42)), ((30, 33), (39, 42)), ((14, 16), (39, 42)), ((1, 2), (39, 42)), ((17, 19), (39, 42)), ((20, 24), (39, 42))}, [], [], [], [], [], {((2, 8), (45, 47)), ((2, 8), (40, 41)), ((2, 8), (55, 57)), ((2, 8), (42, 43)), ((2, 8), (15, 26)), ((15, 26), (40, 41)), ((15, 26), (45, 47)), ((15, 26), (55, 57)), ((15, 26), (42, 43))}, [], [], {((0, 7), (24, 26)), ((0, 7), (13, 18)), ((0, 7), (11, 12))}, {((7, 14), (18, 19))}, [], [], [], [], [], [], {((16, 19), (23, 27))}, {((0, 5), (9, 14))}, {((29, 31), (20, 25))}, [], [], [], [], [], {((32, 39), (43, 46))}, [], {((1, 2), (8, 10)), ((8, 10), (4, 5)), ((1, 2), (4, 5))}, [], {((3, 10), (20, 23)), ((20, 23), (3, 10))}, {((30, 32), (57, 60)), ((10, 11), (12, 16)), ((30, 32), (47, 50)), ((30, 32), (41, 46))}, [], [], [], {((8, 10), (17, 18))}, [], [], {((10, 27), (3, 6))}, [], {((1, 4), (6, 7))}, {((3, 4), (11, 19))}, [], {((22, 24), (7, 12)), ((22, 24), (14, 19))}, [], []]\n"
     ]
    }
   ],
   "source": [
    "print(val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(ref_event, pred_event):\n",
    "    return max(ref_event[0], pred_event[0]) <= min(ref_event[1], pred_event[1])\n",
    "\n",
    "\n",
    "def evaluate_pair(reference, prediction):\n",
    "    ref_cause, ref_effect = reference\n",
    "    pred_cause, pred_effect = prediction\n",
    "    if ref_cause == pred_cause and ref_effect == pred_effect:\n",
    "        return 1\n",
    "    elif overlap(ref_cause, pred_cause) and overlap(ref_effect, pred_effect):\n",
    "        return 0.5\n",
    "    return 0\n",
    "\n",
    "def precision(tp, fp):\n",
    "    if not tp:\n",
    "        return 0\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def recall(tp, fn):\n",
    "    if not tp:\n",
    "        return 0\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def f1(tp, fp, fn):\n",
    "    if not tp:\n",
    "        return 0\n",
    "    return 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "def evaluate(references, predictions):\n",
    "    tps, fps, fns = [], [], []\n",
    "    for reference, prediction in zip(references, predictions):\n",
    "        tp, fp, fn = 0, 0, 0\n",
    "        remaining_references = set(reference)\n",
    "        for pred in prediction:\n",
    "            for ref in remaining_references:\n",
    "                score = evaluate_pair(ref, pred)\n",
    "                if score:\n",
    "                    tp += score\n",
    "                    remaining_references.remove(ref)\n",
    "                    break\n",
    "            else:\n",
    "                fp += 1\n",
    "        fn += len(remaining_references)\n",
    "        tps.append(tp)\n",
    "        fps.append(fp)\n",
    "        fns.append(fn)\n",
    "\n",
    "    macro_prec = sum([precision(tp, fp) for tp, fp in zip(tps, fps)]) / len(tps)\n",
    "    macro_rec = sum([recall(tp, fn) for tp, fn in zip(tps, fns)]) / len(tps)\n",
    "    macro_f1 = sum([f1(tp, fp, fn) for tp, fp, fn in zip(tps, fps, fns)]) / len(tps)\n",
    "    micro_prec = precision(sum(tps), sum(fps))\n",
    "    micro_rec = recall(sum(tps), sum(fns))\n",
    "    micro_f1 = f1(sum(tps), sum(fps), sum(fns))\n",
    "    return {\n",
    "        \"macro\": {\"precision\": macro_prec, \"recall\": macro_rec, \"f1\": macro_f1},\n",
    "        \"micro\": {\"precision\": micro_prec, \"recall\": micro_rec, \"f1\": micro_f1},\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro': {'precision': 0.2774822695035461,\n",
       "  'recall': 0.29476950354609927,\n",
       "  'f1': 0.2816193853427896},\n",
       " 'micro': {'precision': 0.4880952380952381,\n",
       "  'recall': 0.4659090909090909,\n",
       "  'f1': 0.47674418604651164}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(val_labels, val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"./data/teaching-dataset\")\n",
    "with (data_dir / \"relation_extraction_text_test.zip\").open(\"rb\") as file:\n",
    "    zip_file = ZipFile(file)\n",
    "    with zip_file.open(\"input.txt\") as f:\n",
    "        test_sentences = [\n",
    "            sentence.split(\"\\n\") for sentence in f.read().decode(\"utf-8\").split(\"\\n\\n\")\n",
    "        ]\n",
    "\n",
    "pipe = pipeline(sentences, labels)\n",
    "test_predictions = []\n",
    "for s in test_sentences:\n",
    "    test_predictions.append(pipe.predict(s))\n",
    "\n",
    "with open(\"predictions.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(\",\".join(str(relation) for relation in prediction) for prediction in test_predictions).replace(\" \", \"\"))#\n",
    "    \n",
    "# this causes problems if last line is empty!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "print(len(test_sentences))\n",
    "print(len(test_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
