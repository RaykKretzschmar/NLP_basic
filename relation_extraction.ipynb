{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "from tqdm import autonotebook as tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy.tokens import Doc\n",
    "from spacy.matcher import Matcher\n",
    "import re\n",
    "from collections import Counter\n",
    "from collections.abc import Iterable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The longest serving spacecraft goes into retirement . "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "data_dir = Path(\"./data/teaching-dataset\")\n",
    "with (data_dir / \"relation_extraction_text_train.zip\").open(\"rb\") as file:\n",
    "    zip_file = ZipFile(file)\n",
    "    with zip_file.open(\"input.txt\") as f:\n",
    "        sentences = [\n",
    "            sentence.split(\"\\n\") for sentence in f.read().decode(\"utf-8\").split(\"\\n\\n\")\n",
    "        ]\n",
    "with (data_dir / \"relation_extraction_references_train.zip\").open(\"rb\") as file:\n",
    "    zip_file = ZipFile(file)\n",
    "    with zip_file.open(\"references.txt\") as f:\n",
    "        labels = []\n",
    "        for line in f.read().decode(\"utf-8\").split(\"\\n\"):\n",
    "            relations = []\n",
    "            for relation in re.finditer(r\"\\(\\((\\d+),(\\d+)\\),\\((\\d+),(\\d+)\\)\\)\", line):\n",
    "                relation = (\n",
    "                    (int(relation.group(1)), int(relation.group(2))),\n",
    "                    (int(relation.group(3)), int(relation.group(4))),\n",
    "                )\n",
    "                relations.append(relation)\n",
    "            labels.append(relations)\n",
    "assert len(sentences) == len(labels)\n",
    "doc = nlp(Doc(nlp.vocab, words=sentences[0]))\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "train_sentences, test_sentences, train_labels, test_labels =  train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
    "print(len(train_sentences))\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same here\n",
    "class is_causal_predictor:\n",
    "    def __init__(self, sentences, labels, n=None):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.n = n\n",
    "        self.test_sentences = test_sentences\n",
    "        self.test_labels = test_labels\n",
    "        \n",
    "        self.init_words()\n",
    "        if n != None:\n",
    "            self.causal_cues = self.get_causal_cues(self.n)\n",
    "\n",
    "\n",
    "    def init_words(self):\n",
    "        self.words = []\n",
    "        self.nonCausalWords = []\n",
    "\n",
    "        for label, sentence in zip(self.labels, self.sentences):\n",
    "            if type(sentence) == list:\n",
    "                sentence = ' '.join(sentence)\n",
    "            if \"B-EVENT\" in label: #if sentence is causal\n",
    "                doc = nlp(sentence)\n",
    "                wordsHelp = [token.text for token in doc if not token.is_stop and not token.is_punct and token.pos_ != \"NOUN\" and token.pos_ != \"ADJ\"] \n",
    "                self.words.extend(wordsHelp) #append all words to a list if they are NOT nouns & NOT punctuation & NOT adjectives\n",
    "\n",
    "            else: #if sentence is NOT causal\n",
    "                doc = nlp(sentence)\n",
    "                wordsHelp = [token.text for token in doc if not token.is_stop and not token.is_punct and token.pos_ != \"NOUN\" and token.pos_ != \"ADJ\"] \n",
    "                self.nonCausalWords.extend(wordsHelp)  \n",
    "    \n",
    "    # returns n best causal cues\n",
    "    def get_causal_cues(self, n):\n",
    "        def flatten(lis): #pretty ugly solution but we have to flatten the list since every new sentence adds \"[]\" which Counter can't deal with\n",
    "            for item in lis:\n",
    "                if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "                    for x in flatten(item):\n",
    "                        yield x\n",
    "                else:        \n",
    "                    yield item\n",
    "        \n",
    "        def get_n_lemmata(causal_freq, n):\n",
    "            # sort words\n",
    "            sorted_words = np.array(causal_freq.most_common(len(causal_freq)))[:,0]\n",
    "            converted_return = []\n",
    "            for word in sorted_words:\n",
    "                # lemmatize\n",
    "                doc = nlp(str(word))\n",
    "                word = \" \".join([token.lemma_ for token in doc])\n",
    "                if not word in converted_return:\n",
    "                    converted_return.append(word)\n",
    "                # break if n lemmata found\n",
    "                if len(converted_return)==n:\n",
    "                    break\n",
    "            return converted_return\n",
    "\n",
    "        causal_freq = Counter(self.words)\n",
    "        nonCausal_freq = Counter(self.nonCausalWords)\n",
    "        for word in causal_freq:\n",
    "            causal_freq[word] = causal_freq[word]/(nonCausal_freq[word]+1)\n",
    "        return get_n_lemmata(causal_freq, n)\n",
    "\n",
    "\n",
    "    def predict_causality(self, sentence):\n",
    "        if type(sentence) == list: # convert to str\n",
    "            sentence = ' '.join(sentence)\n",
    "        matcher = Matcher(nlp.vocab)\n",
    "        pattern = [[{\"LEMMA\": cue}] for cue in self.causal_cues]\n",
    "        matcher.add(\"CAUSAL\", pattern)\n",
    "        doc = nlp(sentence)\n",
    "        matches = matcher(doc)\n",
    "        return bool(matches)\n",
    "\n",
    "\n",
    "    # Find best value for n given a testset and initialize causal cues according to best n\n",
    "    def init_causal_cues_best_n(self, test_sentences, test_labels, step_size=20):\n",
    "        new_labels = []\n",
    "        for label in test_labels:\n",
    "            if \"B-EVENT\" in label:\n",
    "                new_labels.append(1)\n",
    "            else:\n",
    "                new_labels.append(0)\n",
    "\n",
    "        old_f1 = 0\n",
    "        f1 = 0\n",
    "        n = 0\n",
    "        while f1 >= old_f1:\n",
    "            n = n+step_size\n",
    "            predictions = []\n",
    "\n",
    "            # predict\n",
    "            self.causal_cues = self.get_causal_cues(n)\n",
    "            for sentence, l in zip(test_sentences, test_labels):\n",
    "                p = self.predict_causality(sentence)\n",
    "                predictions.append(p)\n",
    "\n",
    "            # evaluate\n",
    "            old_f1 = f1\n",
    "            tp = sum([int(p) == 1 and int(l) == 1 for p, l in zip(predictions, new_labels)])\n",
    "            fp = sum([int(p) == 1 and int(l) == 0 for p, l in zip(predictions, new_labels)])\n",
    "            fn = sum([int(p) == 0 and int(l) == 1 for p, l in zip(predictions, new_labels)])\n",
    "            precision = tp / (tp + fp)\n",
    "            recall = tp / (tp + fn)\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "        self.f1 = old_f1\n",
    "        self.n = n-20\n",
    "        self.causal_cues = self.get_causal_cues(self.n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
