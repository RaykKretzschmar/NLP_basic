{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "from tqdm import autonotebook as tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy.tokens import Doc\n",
    "import re\n",
    "from collections import Counter\n",
    "from collections.abc import Iterable\n",
    "from spacy.training import biluo_tags_to_spans\n",
    "from itertools import combinations\n",
    "import iobes\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "from seqeval import scheme\n",
    "from tqdm import autonotebook as tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import sklearn_crfsuite\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The longest serving spacecraft goes into retirement . "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "data_dir = Path(\"./data/teaching-dataset\")\n",
    "with (data_dir / \"relation_extraction_text_train.zip\").open(\"rb\") as file:\n",
    "    zip_file = ZipFile(file)\n",
    "    with zip_file.open(\"input.txt\") as f:\n",
    "        sentences = [\n",
    "            sentence.split(\"\\n\") for sentence in f.read().decode(\"utf-8\").split(\"\\n\\n\")\n",
    "        ]\n",
    "with (data_dir / \"relation_extraction_references_train.zip\").open(\"rb\") as file:\n",
    "    zip_file = ZipFile(file)\n",
    "    with zip_file.open(\"references.txt\") as f:\n",
    "        labels = []\n",
    "        for line in f.read().decode(\"utf-8\").split(\"\\n\"):\n",
    "            relations = []\n",
    "            for relation in re.finditer(r\"\\(\\((\\d+),(\\d+)\\),\\((\\d+),(\\d+)\\)\\)\", line):\n",
    "                relation = (\n",
    "                    (int(relation.group(1)), int(relation.group(2))),\n",
    "                    (int(relation.group(3)), int(relation.group(4))),\n",
    "                )\n",
    "                relations.append(relation)\n",
    "            labels.append(relations)\n",
    "assert len(sentences) == len(labels)\n",
    "doc = nlp(Doc(nlp.vocab, words=sentences[0]))\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "train_sentences, test_sentences, train_labels, test_labels =  train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
    "print(len(train_sentences))\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class is_causal_predictor:\n",
    "    def __init__(self, sentences, labels, n=None):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.n = n\n",
    "        \n",
    "        if n != None:\n",
    "            self.init_words(sentences, labels)\n",
    "            self.causal_cues = self.get_causal_cues(self.n)\n",
    "        else:\n",
    "            self.init_causal_cues_best_n(sentences, labels)\n",
    "\n",
    "\n",
    "    def init_words(self, sentences, labels):\n",
    "        self.words = []\n",
    "        self.nonCausalWords = []\n",
    "\n",
    "        for label, sentence in zip(labels, sentences):\n",
    "            if type(sentence) == list:\n",
    "                sentence = ' '.join(sentence)\n",
    "            if label != []: #if sentence is causal\n",
    "                doc = nlp(sentence)\n",
    "                wordsHelp = [token.text for token in doc if not token.is_stop and not token.is_punct and token.pos_ != \"NOUN\" and token.pos_ != \"ADJ\"] \n",
    "                self.words.extend(wordsHelp) #append all words to a list if they are NOT nouns & NOT punctuation & NOT adjectives\n",
    "\n",
    "            else: #if sentence is NOT causal\n",
    "                doc = nlp(sentence)\n",
    "                wordsHelp = [token.text for token in doc if not token.is_stop and not token.is_punct and token.pos_ != \"NOUN\" and token.pos_ != \"ADJ\"] \n",
    "                self.nonCausalWords.extend(wordsHelp)  \n",
    "    \n",
    "\n",
    "    # returns n best causal cues\n",
    "    def get_causal_cues(self, n):\n",
    "        def flatten(lis): #pretty ugly solution but we have to flatten the list since every new sentence adds \"[]\" which Counter can't deal with\n",
    "            for item in lis:\n",
    "                if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "                    for x in flatten(item):\n",
    "                        yield x\n",
    "                else:        \n",
    "                    yield item\n",
    "        \n",
    "        def get_n_lemmata(causal_freq, n):\n",
    "            # sort words\n",
    "            sorted_words = np.array(causal_freq.most_common(len(causal_freq)))[:,0]\n",
    "            converted_return = []\n",
    "            for word in sorted_words:\n",
    "                # lemmatize\n",
    "                doc = nlp(str(word))\n",
    "                word = \" \".join([token.lemma_ for token in doc])\n",
    "                if not word in converted_return:\n",
    "                    converted_return.append(word)\n",
    "                # break if n lemmata found\n",
    "                if len(converted_return)==n:\n",
    "                    break\n",
    "            return converted_return\n",
    "\n",
    "        causal_freq = Counter(self.words)\n",
    "        nonCausal_freq = Counter(self.nonCausalWords)\n",
    "        for word in causal_freq:\n",
    "            causal_freq[word] = causal_freq[word]/(nonCausal_freq[word]+1)\n",
    "        return get_n_lemmata(causal_freq, n)\n",
    "\n",
    "\n",
    "    def predict_causality(self, sentence):\n",
    "        if type(sentence) == list: # convert to str\n",
    "            sentence = ' '.join(sentence)\n",
    "        matcher = Matcher(nlp.vocab)\n",
    "        pattern = [[{\"LEMMA\": cue}] for cue in self.causal_cues]\n",
    "        matcher.add(\"CAUSAL\", pattern)\n",
    "        doc = nlp(sentence)\n",
    "        matches = matcher(doc)\n",
    "        return bool(matches)\n",
    "\n",
    "\n",
    "    # Find best value for n given a testset and initialize causal cues according to best n\n",
    "    def init_causal_cues_best_n(self, sentences, labels, step_size=5):\n",
    "        train_sentences, test_sentences, train_labels, test_labels =  train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
    "        self.init_words(train_sentences, train_labels)\n",
    "\n",
    "        new_labels = []\n",
    "        for label in test_labels:\n",
    "            if label != []:\n",
    "                new_labels.append(1)\n",
    "            else:\n",
    "                new_labels.append(0)\n",
    "\n",
    "        old_f1 = 0\n",
    "        f1 = 0\n",
    "        n = 0\n",
    "        while f1 >= old_f1:\n",
    "            n = n+step_size\n",
    "            predictions = []\n",
    "\n",
    "            # predict\n",
    "            self.causal_cues = self.get_causal_cues(n)\n",
    "            for sentence, l in zip(test_sentences, test_labels):\n",
    "                p = self.predict_causality(sentence)\n",
    "                predictions.append(p)\n",
    "\n",
    "            # evaluate\n",
    "            old_f1 = f1\n",
    "            tp = sum([int(p) == 1 and int(l) == 1 for p, l in zip(predictions, new_labels)])\n",
    "            fp = sum([int(p) == 1 and int(l) == 0 for p, l in zip(predictions, new_labels)])\n",
    "            fn = sum([int(p) == 0 and int(l) == 1 for p, l in zip(predictions, new_labels)])\n",
    "            precision = tp / (tp + fp)\n",
    "            recall = tp / (tp + fn)\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "            x = 0\n",
    "\n",
    "        self.f1 = old_f1\n",
    "        self.n = n - step_size\n",
    "        self.causal_cues = self.get_causal_cues(self.n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, d, t) for w, p, d, t in zip(s['Word'].values.tolist(), \n",
    "                                                           s['POS'].values.tolist(), \n",
    "                                                           s['DEP'].values.tolist(),\n",
    "                                                           s['Tag'].values.tolist())]\n",
    "        self.grouped = self.data.groupby('Sentence #').apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    def get_next(self):\n",
    "        try: \n",
    "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s \n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    deptag = sent[i][2]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0, \n",
    "        'word.lower()': word.lower(), \n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "        'deptag': deptag,\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        deptag1 = sent[i-1][2]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "            '-1:deptag': deptag1,\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        # print(i)\n",
    "        deptag1 = sent[i+1][2]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "            '+1:deptag': deptag1,\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, deptag, label in sent]\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, deptag, label in sent]\n",
    "\n",
    "def flatten(l):\n",
    "    output = []\n",
    "    for sublist in l:\n",
    "        for item in sublist:\n",
    "            output.append(item)\n",
    "\n",
    "        output.append('')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training ML-model with data from exercise 2\n",
    "def train_step_2():\n",
    "    data_dir = Path(\"./data/teaching-dataset\")\n",
    "    with (data_dir / \"span_extraction_text_train.zip\").open(\"rb\") as file:\n",
    "        zip_file = ZipFile(file)\n",
    "        with zip_file.open(\"input.txt\") as f:\n",
    "            sentences = [\n",
    "                sentence.split(\"\\n\") for sentence in f.read().decode(\"utf-8\").split(\"\\n\\n\")\n",
    "            ]\n",
    "    with (data_dir / \"span_extraction_references_train.zip\").open(\"rb\") as file:\n",
    "        zip_file = ZipFile(file)\n",
    "        with zip_file.open(\"references.txt\") as f:\n",
    "            labels = [\n",
    "                sentence.split(\"\\n\") for sentence in f.read().decode(\"utf-8\").split(\"\\n\\n\")\n",
    "            ]\n",
    "\n",
    "    sentence_number=[]\n",
    "    words=[]\n",
    "    tags=[]\n",
    "    dependencies=[]\n",
    "    events=[]\n",
    "    for i in range(len(sentences)):\n",
    "        doc = Doc(nlp.vocab, words=sentences[i])\n",
    "        doc = nlp(doc)\n",
    "        iterator = zip(doc, labels[i])\n",
    "        for token, label in iterator:\n",
    "            sentence_number.append(f'Sentence: {i}')\n",
    "            words.append(str(token))\n",
    "            tags.append(token.pos_)\n",
    "            dependencies.append(token.dep_)\n",
    "            events.append(label)\n",
    "\n",
    "    train_data = {'Sentence #': sentence_number,'Word': words, 'POS': tags, 'DEP': dependencies, 'Tag': events}\n",
    "    df = pd.DataFrame(data=train_data)\n",
    "    # print(df)\n",
    "\n",
    "    df.isnull().sum()\n",
    "\n",
    "    df = df.fillna(method='ffill')\n",
    "    df['Sentence #'].nunique(), df.Word.nunique(), df.Tag.nunique();\n",
    "\n",
    "    df.groupby('Tag').size().reset_index(name='counts');\n",
    "\n",
    "    X = df.drop('Tag', axis=1)\n",
    "    v = DictVectorizer(sparse=False)\n",
    "    X = v.fit_transform(X.to_dict('records'))\n",
    "    y = df.Tag.values\n",
    "    classes = np.unique(y)\n",
    "    classes = classes.tolist()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n",
    "    X_train.shape, y_train.shape;\n",
    "\n",
    "    getter = SentenceGetter(df)\n",
    "    sentences = getter.sentences\n",
    "\n",
    "    X = [sent2features(s) for s in sentences]\n",
    "    y = [sent2labels(s) for s in sentences]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=0)\n",
    "\n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "        algorithm='lbfgs',\n",
    "        c1=0.1,\n",
    "        c2=0.1,\n",
    "        max_iterations=100,\n",
    "        all_possible_transitions=True\n",
    "    )\n",
    "    crf.fit(X_train, y_train);\n",
    "\n",
    "    y_pred = crf.predict(X_test)\n",
    "    # print(metrics.flat_classification_report(y_pred=y_pred, y_true=y_test, labels = new_classes))\n",
    "    # sollte eigentlich funktionieren tuts aber nicht\n",
    "    flat_y_true = flatten(y_test)\n",
    "    flat_y_pred = flatten(y_pred)\n",
    "    print(classification_report(flat_y_true, flat_y_pred))\n",
    "    return(crf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "                   1.00      1.00      1.00         5\n",
      "     B-EVENT       0.73      0.50      0.59        16\n",
      "     I-EVENT       0.59      0.59      0.59        32\n",
      "           O       0.80      0.84      0.82        83\n",
      "\n",
      "    accuracy                           0.75       136\n",
      "   macro avg       0.78      0.73      0.75       136\n",
      "weighted avg       0.75      0.75      0.75       136\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CRF' object has no attribute 'keep_tempfiles'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/IPython/core/formatters.py:974\u001b[0m, in \u001b[0;36mMimeBundleFormatter.__call__\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    971\u001b[0m     method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n\u001b[1;32m    973\u001b[0m     \u001b[39mif\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 974\u001b[0m         \u001b[39mreturn\u001b[39;00m method(include\u001b[39m=\u001b[39;49minclude, exclude\u001b[39m=\u001b[39;49mexclude)\n\u001b[1;32m    975\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:631\u001b[0m, in \u001b[0;36mBaseEstimator._repr_mimebundle_\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_repr_mimebundle_\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    630\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Mime bundle used by jupyter kernels to display estimator\"\"\"\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m     output \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtext/plain\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mrepr\u001b[39;49m(\u001b[39mself\u001b[39;49m)}\n\u001b[1;32m    632\u001b[0m     \u001b[39mif\u001b[39;00m get_config()[\u001b[39m\"\u001b[39m\u001b[39mdisplay\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdiagram\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    633\u001b[0m         output[\u001b[39m\"\u001b[39m\u001b[39mtext/html\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m estimator_html_repr(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:257\u001b[0m, in \u001b[0;36mBaseEstimator.__repr__\u001b[0;34m(self, N_CHAR_MAX)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39m# use ellipsis for sequences with a lot of elements\u001b[39;00m\n\u001b[1;32m    250\u001b[0m pp \u001b[39m=\u001b[39m _EstimatorPrettyPrinter(\n\u001b[1;32m    251\u001b[0m     compact\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    252\u001b[0m     indent\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    253\u001b[0m     indent_at_name\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    254\u001b[0m     n_max_elements_to_show\u001b[39m=\u001b[39mN_MAX_ELEMENTS_TO_SHOW,\n\u001b[1;32m    255\u001b[0m )\n\u001b[0;32m--> 257\u001b[0m repr_ \u001b[39m=\u001b[39m pp\u001b[39m.\u001b[39;49mpformat(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    259\u001b[0m \u001b[39m# Use bruteforce ellipsis when there are a lot of non-blank characters\u001b[39;00m\n\u001b[1;32m    260\u001b[0m n_nonblank \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(repr_\u001b[39m.\u001b[39msplit()))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pprint.py:158\u001b[0m, in \u001b[0;36mPrettyPrinter.pformat\u001b[0;34m(self, object)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpformat\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mobject\u001b[39m):\n\u001b[1;32m    157\u001b[0m     sio \u001b[39m=\u001b[39m _StringIO()\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_format(\u001b[39mobject\u001b[39;49m, sio, \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, {}, \u001b[39m0\u001b[39;49m)\n\u001b[1;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m sio\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pprint.py:175\u001b[0m, in \u001b[0;36mPrettyPrinter._format\u001b[0;34m(self, object, stream, indent, allowance, context, level)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m rep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_repr(\u001b[39mobject\u001b[39;49m, context, level)\n\u001b[1;32m    176\u001b[0m max_width \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_width \u001b[39m-\u001b[39m indent \u001b[39m-\u001b[39m allowance\n\u001b[1;32m    177\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(rep) \u001b[39m>\u001b[39m max_width:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pprint.py:455\u001b[0m, in \u001b[0;36mPrettyPrinter._repr\u001b[0;34m(self, object, context, level)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_repr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mobject\u001b[39m, context, level):\n\u001b[0;32m--> 455\u001b[0m     \u001b[39mrepr\u001b[39m, readable, recursive \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat(\u001b[39mobject\u001b[39;49m, context\u001b[39m.\u001b[39;49mcopy(),\n\u001b[1;32m    456\u001b[0m                                             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_depth, level)\n\u001b[1;32m    457\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m readable:\n\u001b[1;32m    458\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_pprint.py:189\u001b[0m, in \u001b[0;36m_EstimatorPrettyPrinter.format\u001b[0;34m(self, object, context, maxlevels, level)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mobject\u001b[39m, context, maxlevels, level):\n\u001b[0;32m--> 189\u001b[0m     \u001b[39mreturn\u001b[39;00m _safe_repr(\n\u001b[1;32m    190\u001b[0m         \u001b[39mobject\u001b[39;49m, context, maxlevels, level, changed_only\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_changed_only\n\u001b[1;32m    191\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_pprint.py:440\u001b[0m, in \u001b[0;36m_safe_repr\u001b[0;34m(object, context, maxlevels, level, changed_only)\u001b[0m\n\u001b[1;32m    438\u001b[0m recursive \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m changed_only:\n\u001b[0;32m--> 440\u001b[0m     params \u001b[39m=\u001b[39m _changed_params(\u001b[39mobject\u001b[39;49m)\n\u001b[1;32m    441\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39mget_params(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_pprint.py:93\u001b[0m, in \u001b[0;36m_changed_params\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_changed_params\u001b[39m(estimator):\n\u001b[1;32m     90\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return dict (param_name: value) of parameters that were given to\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39m    estimator with non-default values.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     params \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39;49mget_params(deep\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     94\u001b[0m     init_func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(estimator\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdeprecated_original\u001b[39m\u001b[39m\"\u001b[39m, estimator\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m)\n\u001b[1;32m     95\u001b[0m     init_params \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(init_func)\u001b[39m.\u001b[39mparameters\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:170\u001b[0m, in \u001b[0;36mBaseEstimator.get_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    168\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m    169\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_param_names():\n\u001b[0;32m--> 170\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, key)\n\u001b[1;32m    171\u001b[0m     \u001b[39mif\u001b[39;00m deep \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(value, \u001b[39m\"\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mtype\u001b[39m):\n\u001b[1;32m    172\u001b[0m         deep_items \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mget_params()\u001b[39m.\u001b[39mitems()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CRF' object has no attribute 'keep_tempfiles'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CRF' object has no attribute 'keep_tempfiles'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/IPython/core/formatters.py:708\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    701\u001b[0m stream \u001b[39m=\u001b[39m StringIO()\n\u001b[1;32m    702\u001b[0m printer \u001b[39m=\u001b[39m pretty\u001b[39m.\u001b[39mRepresentationPrinter(stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    703\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnewline,\n\u001b[1;32m    704\u001b[0m     max_seq_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_seq_length,\n\u001b[1;32m    705\u001b[0m     singleton_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msingleton_printers,\n\u001b[1;32m    706\u001b[0m     type_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype_printers,\n\u001b[1;32m    707\u001b[0m     deferred_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 708\u001b[0m printer\u001b[39m.\u001b[39;49mpretty(obj)\n\u001b[1;32m    709\u001b[0m printer\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    710\u001b[0m \u001b[39mreturn\u001b[39;00m stream\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[39mreturn\u001b[39;00m meth(obj, \u001b[39mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[39mand\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39m__repr__\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[39mreturn\u001b[39;00m _repr_pprint(obj, \u001b[39mself\u001b[39;49m, cycle)\n\u001b[1;32m    412\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_pprint(obj, \u001b[39mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[39m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mrepr\u001b[39m(obj)\n\u001b[1;32m    779\u001b[0m lines \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[39mwith\u001b[39;00m p\u001b[39m.\u001b[39mgroup():\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:257\u001b[0m, in \u001b[0;36mBaseEstimator.__repr__\u001b[0;34m(self, N_CHAR_MAX)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39m# use ellipsis for sequences with a lot of elements\u001b[39;00m\n\u001b[1;32m    250\u001b[0m pp \u001b[39m=\u001b[39m _EstimatorPrettyPrinter(\n\u001b[1;32m    251\u001b[0m     compact\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    252\u001b[0m     indent\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    253\u001b[0m     indent_at_name\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    254\u001b[0m     n_max_elements_to_show\u001b[39m=\u001b[39mN_MAX_ELEMENTS_TO_SHOW,\n\u001b[1;32m    255\u001b[0m )\n\u001b[0;32m--> 257\u001b[0m repr_ \u001b[39m=\u001b[39m pp\u001b[39m.\u001b[39;49mpformat(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    259\u001b[0m \u001b[39m# Use bruteforce ellipsis when there are a lot of non-blank characters\u001b[39;00m\n\u001b[1;32m    260\u001b[0m n_nonblank \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(repr_\u001b[39m.\u001b[39msplit()))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pprint.py:158\u001b[0m, in \u001b[0;36mPrettyPrinter.pformat\u001b[0;34m(self, object)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpformat\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mobject\u001b[39m):\n\u001b[1;32m    157\u001b[0m     sio \u001b[39m=\u001b[39m _StringIO()\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_format(\u001b[39mobject\u001b[39;49m, sio, \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, {}, \u001b[39m0\u001b[39;49m)\n\u001b[1;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m sio\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pprint.py:175\u001b[0m, in \u001b[0;36mPrettyPrinter._format\u001b[0;34m(self, object, stream, indent, allowance, context, level)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m rep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_repr(\u001b[39mobject\u001b[39;49m, context, level)\n\u001b[1;32m    176\u001b[0m max_width \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_width \u001b[39m-\u001b[39m indent \u001b[39m-\u001b[39m allowance\n\u001b[1;32m    177\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(rep) \u001b[39m>\u001b[39m max_width:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pprint.py:455\u001b[0m, in \u001b[0;36mPrettyPrinter._repr\u001b[0;34m(self, object, context, level)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_repr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mobject\u001b[39m, context, level):\n\u001b[0;32m--> 455\u001b[0m     \u001b[39mrepr\u001b[39m, readable, recursive \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat(\u001b[39mobject\u001b[39;49m, context\u001b[39m.\u001b[39;49mcopy(),\n\u001b[1;32m    456\u001b[0m                                             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_depth, level)\n\u001b[1;32m    457\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m readable:\n\u001b[1;32m    458\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_pprint.py:189\u001b[0m, in \u001b[0;36m_EstimatorPrettyPrinter.format\u001b[0;34m(self, object, context, maxlevels, level)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mobject\u001b[39m, context, maxlevels, level):\n\u001b[0;32m--> 189\u001b[0m     \u001b[39mreturn\u001b[39;00m _safe_repr(\n\u001b[1;32m    190\u001b[0m         \u001b[39mobject\u001b[39;49m, context, maxlevels, level, changed_only\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_changed_only\n\u001b[1;32m    191\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_pprint.py:440\u001b[0m, in \u001b[0;36m_safe_repr\u001b[0;34m(object, context, maxlevels, level, changed_only)\u001b[0m\n\u001b[1;32m    438\u001b[0m recursive \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m changed_only:\n\u001b[0;32m--> 440\u001b[0m     params \u001b[39m=\u001b[39m _changed_params(\u001b[39mobject\u001b[39;49m)\n\u001b[1;32m    441\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39mget_params(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_pprint.py:93\u001b[0m, in \u001b[0;36m_changed_params\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_changed_params\u001b[39m(estimator):\n\u001b[1;32m     90\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return dict (param_name: value) of parameters that were given to\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39m    estimator with non-default values.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     params \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39;49mget_params(deep\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     94\u001b[0m     init_func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(estimator\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdeprecated_original\u001b[39m\u001b[39m\"\u001b[39m, estimator\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m)\n\u001b[1;32m     95\u001b[0m     init_params \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(init_func)\u001b[39m.\u001b[39mparameters\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:170\u001b[0m, in \u001b[0;36mBaseEstimator.get_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    168\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m    169\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_param_names():\n\u001b[0;32m--> 170\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, key)\n\u001b[1;32m    171\u001b[0m     \u001b[39mif\u001b[39;00m deep \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(value, \u001b[39m\"\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mtype\u001b[39m):\n\u001b[1;32m    172\u001b[0m         deep_items \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mget_params()\u001b[39m.\u001b[39mitems()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CRF' object has no attribute 'keep_tempfiles'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CRF' object has no attribute 'keep_tempfiles'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/IPython/core/formatters.py:344\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    342\u001b[0m     method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n\u001b[1;32m    343\u001b[0m     \u001b[39mif\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m         \u001b[39mreturn\u001b[39;00m method()\n\u001b[1;32m    345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:627\u001b[0m, in \u001b[0;36mBaseEstimator._repr_html_inner\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_repr_html_inner\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    623\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This function is returned by the @property `_repr_html_` to make\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[39m    `hasattr(estimator, \"_repr_html_\") return `True` or `False` depending\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \u001b[39m    on `get_config()[\"display\"]`.\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 627\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator_html_repr(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_estimator_html_repr.py:393\u001b[0m, in \u001b[0;36mestimator_html_repr\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m    391\u001b[0m style_template \u001b[39m=\u001b[39m Template(_STYLE)\n\u001b[1;32m    392\u001b[0m style_with_id \u001b[39m=\u001b[39m style_template\u001b[39m.\u001b[39msubstitute(\u001b[39mid\u001b[39m\u001b[39m=\u001b[39mcontainer_id)\n\u001b[0;32m--> 393\u001b[0m estimator_str \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39;49m(estimator)\n\u001b[1;32m    395\u001b[0m \u001b[39m# The fallback message is shown by default and loading the CSS sets\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[39m# div.sk-text-repr-fallback to display: none to hide the fallback message.\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39m# The reverse logic applies to HTML repr div.sk-container.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[39m# div.sk-container is hidden by default and the loading the CSS displays it.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m fallback_msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    405\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mIn a Jupyter environment, please rerun this cell to show the HTML\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m representation or trust the notebook. <br />On GitHub, the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m HTML representation is unable to render, please try loading this page\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m with nbviewer.org.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:257\u001b[0m, in \u001b[0;36mBaseEstimator.__repr__\u001b[0;34m(self, N_CHAR_MAX)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39m# use ellipsis for sequences with a lot of elements\u001b[39;00m\n\u001b[1;32m    250\u001b[0m pp \u001b[39m=\u001b[39m _EstimatorPrettyPrinter(\n\u001b[1;32m    251\u001b[0m     compact\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    252\u001b[0m     indent\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    253\u001b[0m     indent_at_name\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    254\u001b[0m     n_max_elements_to_show\u001b[39m=\u001b[39mN_MAX_ELEMENTS_TO_SHOW,\n\u001b[1;32m    255\u001b[0m )\n\u001b[0;32m--> 257\u001b[0m repr_ \u001b[39m=\u001b[39m pp\u001b[39m.\u001b[39;49mpformat(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    259\u001b[0m \u001b[39m# Use bruteforce ellipsis when there are a lot of non-blank characters\u001b[39;00m\n\u001b[1;32m    260\u001b[0m n_nonblank \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(repr_\u001b[39m.\u001b[39msplit()))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pprint.py:158\u001b[0m, in \u001b[0;36mPrettyPrinter.pformat\u001b[0;34m(self, object)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpformat\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mobject\u001b[39m):\n\u001b[1;32m    157\u001b[0m     sio \u001b[39m=\u001b[39m _StringIO()\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_format(\u001b[39mobject\u001b[39;49m, sio, \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, {}, \u001b[39m0\u001b[39;49m)\n\u001b[1;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m sio\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pprint.py:175\u001b[0m, in \u001b[0;36mPrettyPrinter._format\u001b[0;34m(self, object, stream, indent, allowance, context, level)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m rep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_repr(\u001b[39mobject\u001b[39;49m, context, level)\n\u001b[1;32m    176\u001b[0m max_width \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_width \u001b[39m-\u001b[39m indent \u001b[39m-\u001b[39m allowance\n\u001b[1;32m    177\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(rep) \u001b[39m>\u001b[39m max_width:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pprint.py:455\u001b[0m, in \u001b[0;36mPrettyPrinter._repr\u001b[0;34m(self, object, context, level)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_repr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mobject\u001b[39m, context, level):\n\u001b[0;32m--> 455\u001b[0m     \u001b[39mrepr\u001b[39m, readable, recursive \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat(\u001b[39mobject\u001b[39;49m, context\u001b[39m.\u001b[39;49mcopy(),\n\u001b[1;32m    456\u001b[0m                                             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_depth, level)\n\u001b[1;32m    457\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m readable:\n\u001b[1;32m    458\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_pprint.py:189\u001b[0m, in \u001b[0;36m_EstimatorPrettyPrinter.format\u001b[0;34m(self, object, context, maxlevels, level)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mobject\u001b[39m, context, maxlevels, level):\n\u001b[0;32m--> 189\u001b[0m     \u001b[39mreturn\u001b[39;00m _safe_repr(\n\u001b[1;32m    190\u001b[0m         \u001b[39mobject\u001b[39;49m, context, maxlevels, level, changed_only\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_changed_only\n\u001b[1;32m    191\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_pprint.py:440\u001b[0m, in \u001b[0;36m_safe_repr\u001b[0;34m(object, context, maxlevels, level, changed_only)\u001b[0m\n\u001b[1;32m    438\u001b[0m recursive \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m changed_only:\n\u001b[0;32m--> 440\u001b[0m     params \u001b[39m=\u001b[39m _changed_params(\u001b[39mobject\u001b[39;49m)\n\u001b[1;32m    441\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39mget_params(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_pprint.py:93\u001b[0m, in \u001b[0;36m_changed_params\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_changed_params\u001b[39m(estimator):\n\u001b[1;32m     90\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return dict (param_name: value) of parameters that were given to\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39m    estimator with non-default values.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     params \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39;49mget_params(deep\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     94\u001b[0m     init_func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(estimator\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdeprecated_original\u001b[39m\u001b[39m\"\u001b[39m, estimator\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m)\n\u001b[1;32m     95\u001b[0m     init_params \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(init_func)\u001b[39m.\u001b[39mparameters\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:170\u001b[0m, in \u001b[0;36mBaseEstimator.get_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    168\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m    169\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_param_names():\n\u001b[0;32m--> 170\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, key)\n\u001b[1;32m    171\u001b[0m     \u001b[39mif\u001b[39;00m deep \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(value, \u001b[39m\"\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mtype\u001b[39m):\n\u001b[1;32m    172\u001b[0m         deep_items \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mget_params()\u001b[39m.\u001b[39mitems()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CRF' object has no attribute 'keep_tempfiles'"
     ]
    }
   ],
   "source": [
    "train_step_2()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, d) for w, p, d in zip(s['Word'].values.tolist(), \n",
    "                                                           s['POS'].values.tolist(), \n",
    "                                                           s['DEP'].values.tolist())]\n",
    "        self.grouped = self.data.groupby('Sentence #').apply(agg_func)\n",
    "        \n",
    "\n",
    "        # print(self.grouped.head(20))\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "        \n",
    "    def get_next(self):\n",
    "        try: \n",
    "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s \n",
    "        except:\n",
    "            return None\n",
    "\n",
    "class span_extractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def create_dataframe(self, test_sentences):\n",
    "        sentence_number=[]\n",
    "        words=[]\n",
    "        tags=[]\n",
    "        dependencies=[]\n",
    "        events=[]\n",
    "        for i in range(len(test_sentences)):\n",
    "            doc = Doc(nlp.vocab, words=test_sentences[i])\n",
    "            doc = nlp(doc)\n",
    "            for token in doc:\n",
    "                sentence_number.append(f'Sentence: {i}')\n",
    "                words.append(str(token))\n",
    "                tags.append(token.pos_)\n",
    "                dependencies.append(token.dep_)\n",
    "\n",
    "        train_data = {'Sentence #': sentence_number,'Word': words, 'POS': tags, 'DEP': dependencies}\n",
    "        df = pd.DataFrame(data=train_data)\n",
    "        # print(df)\n",
    "\n",
    "    def getSentences(self, df):\n",
    "        getter = SentenceGetter(df)\n",
    "        sentences = getter.sentences\n",
    "        # for sentence in sentences:\n",
    "        #     print(sentence)\n",
    "\n",
    "    def predict(self, sentences, crf):\n",
    "        X = [sent2features(s) for s in sentences]\n",
    "        y_pred = crf.predict(X)\n",
    "        # print(y_pred)\n",
    "        flat_y_pred = flatten(y_pred)\n",
    "\n",
    "        return flat_y_pred\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relation_classificator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def parse_sentence(self, sentence, spans):\n",
    "        words = []\n",
    "        tags = []\n",
    "        for word, tag in zip(sentence, spans):\n",
    "            words.append(word)\n",
    "            tags.append(tag)\n",
    "        doc = Doc(nlp.vocab, words=words)\n",
    "        doc = nlp(doc)\n",
    "        tags = iobes.bio_to_bilou(tags)\n",
    "        doc.ents = biluo_tags_to_spans(doc, tags)\n",
    "        return doc\n",
    "\n",
    "    def token_in_between_events(self, token, outer1, outer2):\n",
    "        return  token.i <= outer1.start and token.i >= outer2.end or \\\n",
    "                token.i <= outer2.start and token.i >= outer1.end\n",
    "\n",
    "    def inside_event(self, event, token):\n",
    "        return event.start <= token.i and event.end >= token.i\n",
    "\n",
    "    def events_inside_subtree(self, verb, event1, event2):\n",
    "        a = False\n",
    "        b = False\n",
    "        for sub in verb.subtree:\n",
    "            if self.inside_event(event1, sub):\n",
    "                a = True\n",
    "        for sub in verb.subtree:\n",
    "            if self.inside_event(event2, sub):\n",
    "                b = True\n",
    "        return a and b\n",
    "\n",
    "    def backwards(self, verb, doc):\n",
    "        keyword = None\n",
    "        backwards = False\n",
    "        for child in verb.children:\n",
    "            if child.dep_ in [\"agent\"]:\n",
    "                backwards = True\n",
    "                keyword = child\n",
    "            if child.text in [\"from\"]:\n",
    "                backwards = True\n",
    "                keyword = child\n",
    "        next_word = doc[verb.i+1]\n",
    "        if next_word.dep_ in [\"aux\"]:\n",
    "            backwards = True\n",
    "            keyword = next_word\n",
    "        return backwards, keyword\n",
    "\n",
    "    def get_next_event(self, doc, position):\n",
    "        lowest_distance = float('inf')\n",
    "        for event in doc.ents:\n",
    "            if abs(event.start - position) <= lowest_distance and position > event.end or position < event.start:\n",
    "                next_event = event\n",
    "                lowest_distance = abs(event.start - position)\n",
    "                return next_event\n",
    "\n",
    "    def handle_cause_of(self, predictions, doc):\n",
    "        for token in doc:\n",
    "            if token.text == \"cause\" and doc[token.i+1].text in [\"of\", \"for\"]:\n",
    "                for event1, event2 in combinations(doc.ents, 2):\n",
    "                    if self.token_in_between_events(token, event1, event2):\n",
    "                        predictions.append(((event1.start, event1.end), (event2.start, event2.end)))\n",
    "                if len(predictions) == 0:\n",
    "                    effect = self.get_next_event(doc, token.i)\n",
    "                    cause = self.get_next_event(doc, effect.end)\n",
    "                    predictions.append(((cause.start, cause.end), (effect.start, effect.end)))\n",
    "        return predictions\n",
    "\n",
    "    def handle_because(self, predictions, doc):\n",
    "        for token in doc:\n",
    "            if token.text == \"because\" or token.text == \"due\" or token.text == \"common\" and doc[token.i+1].text == \"with\":\n",
    "                for event1, event2 in combinations(doc.ents, 2):\n",
    "                    if self.token_in_between_events(token, event1, event2):\n",
    "                        predictions.append(((event2.start, event2.end), (event1.start, event1.end)))\n",
    "                if len(predictions) == 0:\n",
    "                    effect = self.get_next_event(doc, token.i)\n",
    "                    cause = self.get_next_event(doc, effect.end)\n",
    "                    predictions.append(((cause.start, cause.end), (effect.start, effect.end)))\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def predict(self, sentence, spans):\n",
    "        doc = self.parse_sentence(sentence, spans)\n",
    "        events = doc.ents\n",
    "        predictions = []\n",
    "\n",
    "        predictions = self.handle_cause_of(predictions, doc)\n",
    "        predictions = self.handle_because(predictions, doc)\n",
    "\n",
    "        # find verbs\n",
    "        verb_pattern = [[{'POS': 'VERB'}]]\n",
    "        verb_matcher = Matcher(nlp.vocab)\n",
    "        verb_matcher.add(\"verbs\", verb_pattern)\n",
    "        matches = verb_matcher(doc)\n",
    "        verbs = [(doc[start:end]) for _, start, end in matches]\n",
    "        verbs = [doc[verb.start] for verb in verbs] # get tokens instead of spans\n",
    "\n",
    "        # remove verbs inside events\n",
    "        for verb in verbs:\n",
    "            for event in events:\n",
    "                if verb.i >= event.start and verb.i < event.end:\n",
    "                    verbs.remove(verb)\n",
    "\n",
    "        for event1, event2 in combinations(doc.ents, 2): # events are actually in order\n",
    "            for verb in verbs:\n",
    "                # find out if backwards relation is given\n",
    "                is_backwards, keyword = self.backwards(verb, doc)\n",
    "                # predict if both events inside the verb's subtree and the verb is in between events\n",
    "                if is_backwards:\n",
    "                    if self.events_inside_subtree(verb, event1, event2) and self.token_in_between_events(keyword, event1, event2):\n",
    "                        predictions.append(((event2.start, event2.end), (event1.start, event1.end)))\n",
    "                else:\n",
    "                    if self.events_inside_subtree(verb, event1, event2) and self.token_in_between_events(verb, event1, event2):\n",
    "                        predictions.append(((event1.start, event1.end), (event2.start, event2.end)))\n",
    "        \n",
    "        # if there is no prediction yet, use a less strict rule\n",
    "        if len(predictions) == 0:\n",
    "            for event1, event2 in combinations(doc.ents, 2):\n",
    "                for verb in verbs:\n",
    "                    # find out if backwards relation is given\n",
    "                    is_backwards, keyword = self.backwards(verb, doc)\n",
    "                    # predict if the verb is in between events - less strict\n",
    "                    if is_backwards: \n",
    "                        if self.token_in_between_events(keyword, event1, event2):\n",
    "                            predictions.append(((event2.start, event2.end), (event1.start, event1.end)))\n",
    "                    else:\n",
    "                        if self.token_in_between_events(verb, event1, event2):\n",
    "                            predictions.append(((event1.start, event1.end), (event2.start, event2.end)))\n",
    "\n",
    "        if len(predictions) == 0:\n",
    "            for event1, event2 in combinations(doc.ents, 2):\n",
    "                predictions.append(((event1.start, event1.end), (event2.start, event2.end)))\n",
    "\n",
    "        predictions = set(predictions)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pipeline:\n",
    "    def __init__(self, train_sentences, train_labels) :\n",
    "        self.train_sentences = train_sentences\n",
    "        self.train_labels = train_labels\n",
    "        \n",
    "        self.is_causal_predictor = is_causal_predictor(train_sentences,train_labels, n=20)\n",
    "        self.span_extractor = span_extractor()\n",
    "        self.relation_classificator = relation_classificator()\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        is_causal = self.is_causal_predictor.predict_causality(sentence)\n",
    "        if is_causal:\n",
    "            spans = self.span_extractor.predict(sentence, train_step_2())\n",
    "            prediction = self.relation_classificator.predict(sentence, spans)\n",
    "        else:\n",
    "            prediction = []\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['associate',\n",
       " 'cause',\n",
       " 'lead',\n",
       " 'include',\n",
       " 'increase',\n",
       " 'AGEP',\n",
       " 'result',\n",
       " 'induce',\n",
       " 'relate',\n",
       " 'Haiti',\n",
       " 'hht',\n",
       " 'commonly',\n",
       " 'occur',\n",
       " 'silica',\n",
       " 'approximately',\n",
       " 'uroporphyrinogen',\n",
       " 'develop',\n",
       " 'particularly',\n",
       " 'enamel',\n",
       " 'call']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(train_sentences, train_labels)\n",
    "pipe.is_causal_predictor.causal_cues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "                   1.00      1.00      1.00         5\n",
      "     B-EVENT       0.73      0.50      0.59        16\n",
      "     I-EVENT       0.59      0.59      0.59        32\n",
      "           O       0.80      0.84      0.82        83\n",
      "\n",
      "    accuracy                           0.75       136\n",
      "   macro avg       0.78      0.73      0.75       136\n",
      "weighted avg       0.75      0.75      0.75       136\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipe\u001b[39m.\u001b[39;49mpredict(sentences[\u001b[39m11\u001b[39;49m])\n",
      "Cell \u001b[0;32mIn[71], line 13\u001b[0m, in \u001b[0;36mpipeline.predict\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     11\u001b[0m is_causal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_causal_predictor\u001b[39m.\u001b[39mpredict_causality(sentence)\n\u001b[1;32m     12\u001b[0m \u001b[39mif\u001b[39;00m is_causal:\n\u001b[0;32m---> 13\u001b[0m     spans \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspan_extractor\u001b[39m.\u001b[39;49mpredict(sentence, train_step_2())\n\u001b[1;32m     14\u001b[0m     prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelation_classificator\u001b[39m.\u001b[39mpredict(sentence, spans)\n\u001b[1;32m     15\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[56], line 55\u001b[0m, in \u001b[0;36mspan_extractor.predict\u001b[0;34m(self, sentences, crf)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, sentences, crf):\n\u001b[0;32m---> 55\u001b[0m     X \u001b[39m=\u001b[39m [sent2features(s) \u001b[39mfor\u001b[39;49;00m s \u001b[39min\u001b[39;49;00m sentences]\n\u001b[1;32m     56\u001b[0m     y_pred \u001b[39m=\u001b[39m crf\u001b[39m.\u001b[39mpredict(X)\n\u001b[1;32m     57\u001b[0m     \u001b[39m# print(y_pred)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[56], line 55\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, sentences, crf):\n\u001b[0;32m---> 55\u001b[0m     X \u001b[39m=\u001b[39m [sent2features(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m sentences]\n\u001b[1;32m     56\u001b[0m     y_pred \u001b[39m=\u001b[39m crf\u001b[39m.\u001b[39mpredict(X)\n\u001b[1;32m     57\u001b[0m     \u001b[39m# print(y_pred)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[68], line 51\u001b[0m, in \u001b[0;36msent2features\u001b[0;34m(sent)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent2features\u001b[39m(sent):\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m [word2features(sent, i) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(sent))]\n",
      "Cell \u001b[0;32mIn[68], line 51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent2features\u001b[39m(sent):\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m [word2features(sent, i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(sent))]\n",
      "Cell \u001b[0;32mIn[68], line 3\u001b[0m, in \u001b[0;36mword2features\u001b[0;34m(sent, i)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword2features\u001b[39m(sent, i):\n\u001b[1;32m      2\u001b[0m     word \u001b[39m=\u001b[39m sent[i][\u001b[39m0\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m     postag \u001b[39m=\u001b[39m sent[i][\u001b[39m1\u001b[39;49m]\n\u001b[1;32m      4\u001b[0m     deptag \u001b[39m=\u001b[39m sent[i][\u001b[39m2\u001b[39m]\n\u001b[1;32m      6\u001b[0m     features \u001b[39m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1.0\u001b[39m, \n\u001b[1;32m      8\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mword.lower()\u001b[39m\u001b[39m'\u001b[39m: word\u001b[39m.\u001b[39mlower(), \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdeptag\u001b[39m\u001b[39m'\u001b[39m: deptag,\n\u001b[1;32m     17\u001b[0m     }\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "pipe.predict(sentences[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
