{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "from tqdm import autonotebook as tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy.tokens import Doc\n",
    "from spacy.matcher import Matcher\n",
    "import re\n",
    "from collections import Counter\n",
    "from collections.abc import Iterable\n",
    "from spacy.training import biluo_tags_to_spans\n",
    "from itertools import combinations\n",
    "import iobes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The longest serving spacecraft goes into retirement . "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "data_dir = Path(\"./data/teaching-dataset\")\n",
    "with (data_dir / \"relation_extraction_text_train.zip\").open(\"rb\") as file:\n",
    "    zip_file = ZipFile(file)\n",
    "    with zip_file.open(\"input.txt\") as f:\n",
    "        sentences = [\n",
    "            sentence.split(\"\\n\") for sentence in f.read().decode(\"utf-8\").split(\"\\n\\n\")\n",
    "        ]\n",
    "with (data_dir / \"relation_extraction_references_train.zip\").open(\"rb\") as file:\n",
    "    zip_file = ZipFile(file)\n",
    "    with zip_file.open(\"references.txt\") as f:\n",
    "        labels = []\n",
    "        for line in f.read().decode(\"utf-8\").split(\"\\n\"):\n",
    "            relations = []\n",
    "            for relation in re.finditer(r\"\\(\\((\\d+),(\\d+)\\),\\((\\d+),(\\d+)\\)\\)\", line):\n",
    "                relation = (\n",
    "                    (int(relation.group(1)), int(relation.group(2))),\n",
    "                    (int(relation.group(3)), int(relation.group(4))),\n",
    "                )\n",
    "                relations.append(relation)\n",
    "            labels.append(relations)\n",
    "assert len(sentences) == len(labels)\n",
    "doc = nlp(Doc(nlp.vocab, words=sentences[0]))\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "train_sentences, test_sentences, train_labels, test_labels =  train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
    "print(len(train_sentences))\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class is_causal_predictor:\n",
    "    def __init__(self, sentences, labels, n=None):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.n = n\n",
    "        \n",
    "        if n != None:\n",
    "            self.init_words(sentences, labels)\n",
    "            self.causal_cues = self.get_causal_cues(self.n)\n",
    "        else:\n",
    "            self.init_causal_cues_best_n(sentences, labels)\n",
    "\n",
    "\n",
    "    def init_words(self, sentences, labels):\n",
    "        self.words = []\n",
    "        self.nonCausalWords = []\n",
    "\n",
    "        for label, sentence in zip(labels, sentences):\n",
    "            if type(sentence) == list:\n",
    "                sentence = ' '.join(sentence)\n",
    "            if label != []: #if sentence is causal\n",
    "                doc = nlp(sentence)\n",
    "                wordsHelp = [token.text for token in doc if not token.is_stop and not token.is_punct and token.pos_ != \"NOUN\" and token.pos_ != \"ADJ\"] \n",
    "                self.words.extend(wordsHelp) #append all words to a list if they are NOT nouns & NOT punctuation & NOT adjectives\n",
    "\n",
    "            else: #if sentence is NOT causal\n",
    "                doc = nlp(sentence)\n",
    "                wordsHelp = [token.text for token in doc if not token.is_stop and not token.is_punct and token.pos_ != \"NOUN\" and token.pos_ != \"ADJ\"] \n",
    "                self.nonCausalWords.extend(wordsHelp)  \n",
    "    \n",
    "\n",
    "    # returns n best causal cues\n",
    "    def get_causal_cues(self, n):\n",
    "        def flatten(lis): #pretty ugly solution but we have to flatten the list since every new sentence adds \"[]\" which Counter can't deal with\n",
    "            for item in lis:\n",
    "                if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "                    for x in flatten(item):\n",
    "                        yield x\n",
    "                else:        \n",
    "                    yield item\n",
    "        \n",
    "        def get_n_lemmata(causal_freq, n):\n",
    "            # sort words\n",
    "            sorted_words = np.array(causal_freq.most_common(len(causal_freq)))[:,0]\n",
    "            converted_return = []\n",
    "            for word in sorted_words:\n",
    "                # lemmatize\n",
    "                doc = nlp(str(word))\n",
    "                word = \" \".join([token.lemma_ for token in doc])\n",
    "                if not word in converted_return:\n",
    "                    converted_return.append(word)\n",
    "                # break if n lemmata found\n",
    "                if len(converted_return)==n:\n",
    "                    break\n",
    "            return converted_return\n",
    "\n",
    "        causal_freq = Counter(self.words)\n",
    "        nonCausal_freq = Counter(self.nonCausalWords)\n",
    "        for word in causal_freq:\n",
    "            causal_freq[word] = causal_freq[word]/(nonCausal_freq[word]+1)\n",
    "        return get_n_lemmata(causal_freq, n)\n",
    "\n",
    "\n",
    "    def predict_causality(self, sentence):\n",
    "        if type(sentence) == list: # convert to str\n",
    "            sentence = ' '.join(sentence)\n",
    "        matcher = Matcher(nlp.vocab)\n",
    "        pattern = [[{\"LEMMA\": cue}] for cue in self.causal_cues]\n",
    "        matcher.add(\"CAUSAL\", pattern)\n",
    "        doc = nlp(sentence)\n",
    "        matches = matcher(doc)\n",
    "        return bool(matches)\n",
    "\n",
    "\n",
    "    # Find best value for n given a testset and initialize causal cues according to best n\n",
    "    def init_causal_cues_best_n(self, sentences, labels, step_size=5):\n",
    "        train_sentences, test_sentences, train_labels, test_labels =  train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
    "        self.init_words(train_sentences, train_labels)\n",
    "\n",
    "        new_labels = []\n",
    "        for label in test_labels:\n",
    "            if label != []:\n",
    "                new_labels.append(1)\n",
    "            else:\n",
    "                new_labels.append(0)\n",
    "\n",
    "        old_f1 = 0\n",
    "        f1 = 0\n",
    "        n = 0\n",
    "        while f1 >= old_f1:\n",
    "            n = n+step_size\n",
    "            predictions = []\n",
    "\n",
    "            # predict\n",
    "            self.causal_cues = self.get_causal_cues(n)\n",
    "            for sentence, l in zip(test_sentences, test_labels):\n",
    "                p = self.predict_causality(sentence)\n",
    "                predictions.append(p)\n",
    "\n",
    "            # evaluate\n",
    "            old_f1 = f1\n",
    "            tp = sum([int(p) == 1 and int(l) == 1 for p, l in zip(predictions, new_labels)])\n",
    "            fp = sum([int(p) == 1 and int(l) == 0 for p, l in zip(predictions, new_labels)])\n",
    "            fn = sum([int(p) == 0 and int(l) == 1 for p, l in zip(predictions, new_labels)])\n",
    "            precision = tp / (tp + fp)\n",
    "            recall = tp / (tp + fn)\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "            x = 0\n",
    "\n",
    "        self.f1 = old_f1\n",
    "        self.n = n - step_size\n",
    "        self.causal_cues = self.get_causal_cues(self.n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relation_classificator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def parse_sentence(self, sentence, spans):\n",
    "        words = []\n",
    "        tags = []\n",
    "        for word, tag in zip(sentence, spans):\n",
    "            words.append(word)\n",
    "            tags.append(tag)\n",
    "        doc = Doc(nlp.vocab, words=words)\n",
    "        doc = nlp(doc)\n",
    "        tags = iobes.bio_to_bilou(tags)\n",
    "        doc.ents = biluo_tags_to_spans(doc, tags)\n",
    "        return doc\n",
    "\n",
    "    def token_in_between_events(self, token, outer1, outer2):\n",
    "        return  token.i <= outer1.start and token.i >= outer2.end or \\\n",
    "                token.i <= outer2.start and token.i >= outer1.end\n",
    "\n",
    "    def inside_event(self, event, token):\n",
    "        return event.start <= token.i and event.end >= token.i\n",
    "\n",
    "    def events_inside_subtree(self, verb, event1, event2):\n",
    "        a = False\n",
    "        b = False\n",
    "        for sub in verb.subtree:\n",
    "            if self.inside_event(event1, sub):\n",
    "                a = True\n",
    "        for sub in verb.subtree:\n",
    "            if self.inside_event(event2, sub):\n",
    "                b = True\n",
    "        return a and b\n",
    "\n",
    "    def backwards(self, verb, doc):\n",
    "        keyword = None\n",
    "        backwards = False\n",
    "        for child in verb.children:\n",
    "            if child.dep_ in [\"agent\"]:\n",
    "                backwards = True\n",
    "                keyword = child\n",
    "            if child.text in [\"from\"]:\n",
    "                backwards = True\n",
    "                keyword = child\n",
    "        next_word = doc[verb.i+1]\n",
    "        if next_word.dep_ in [\"aux\"]:\n",
    "            backwards = True\n",
    "            keyword = next_word\n",
    "        return backwards, keyword\n",
    "\n",
    "    def get_next_event(self, doc, position):\n",
    "        lowest_distance = float('inf')\n",
    "        for event in doc.ents:\n",
    "            if abs(event.start - position) <= lowest_distance and position > event.end or position < event.start:\n",
    "                next_event = event\n",
    "                lowest_distance = abs(event.start - position)\n",
    "                return next_event\n",
    "\n",
    "    def handle_cause_of(self, predictions, doc):\n",
    "        for token in doc:\n",
    "            if token.text == \"cause\" and doc[token.i+1].text in [\"of\", \"for\"]:\n",
    "                for event1, event2 in combinations(doc.ents, 2):\n",
    "                    if self.token_in_between_events(token, event1, event2):\n",
    "                        predictions.append(((event1.start, event1.end), (event2.start, event2.end)))\n",
    "                if len(predictions) == 0:\n",
    "                    effect = self.get_next_event(doc, token.i)\n",
    "                    cause = self.get_next_event(doc, effect.end)\n",
    "                    predictions.append(((cause.start, cause.end), (effect.start, effect.end)))\n",
    "        return predictions\n",
    "\n",
    "    def handle_because(self, predictions, doc):\n",
    "        for token in doc:\n",
    "            if token.text == \"because\" or token.text == \"due\" or token.text == \"common\" and doc[token.i+1].text == \"with\":\n",
    "                for event1, event2 in combinations(doc.ents, 2):\n",
    "                    if self.token_in_between_events(token, event1, event2):\n",
    "                        predictions.append(((event2.start, event2.end), (event1.start, event1.end)))\n",
    "                if len(predictions) == 0:\n",
    "                    effect = self.get_next_event(doc, token.i)\n",
    "                    cause = self.get_next_event(doc, effect.end)\n",
    "                    predictions.append(((cause.start, cause.end), (effect.start, effect.end)))\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def predict(self, sentence, spans):\n",
    "        doc = self.parse_sentence(sentence, spans)\n",
    "        events = doc.ents\n",
    "        predictions = []\n",
    "\n",
    "        predictions = self.handle_cause_of(predictions, doc)\n",
    "        predictions = self.handle_because(predictions, doc)\n",
    "\n",
    "        # find verbs\n",
    "        verb_pattern = [[{'POS': 'VERB'}]]\n",
    "        verb_matcher = Matcher(nlp.vocab)\n",
    "        verb_matcher.add(\"verbs\", verb_pattern)\n",
    "        matches = verb_matcher(doc)\n",
    "        verbs = [(doc[start:end]) for _, start, end in matches]\n",
    "        verbs = [doc[verb.start] for verb in verbs] # get tokens instead of spans\n",
    "\n",
    "        # remove verbs inside events\n",
    "        for verb in verbs:\n",
    "            for event in events:\n",
    "                if verb.i >= event.start and verb.i < event.end:\n",
    "                    verbs.remove(verb)\n",
    "\n",
    "        for event1, event2 in combinations(doc.ents, 2):\n",
    "            # entities are actually in order\n",
    "            for verb in verbs:\n",
    "                is_backwards, keyword = self.backwards(verb, doc)\n",
    "                if is_backwards:\n",
    "                    if self.events_inside_subtree(verb, event1, event2) and self.token_in_between_events(keyword, event1, event2):\n",
    "                        predictions.append(((event2.start, event2.end), (event1.start, event1.end)))\n",
    "                else:\n",
    "                    if self.events_inside_subtree(verb, event1, event2) and self.token_in_between_events(verb, event1, event2):\n",
    "                        predictions.append(((event1.start, event1.end), (event2.start, event2.end)))\n",
    "\n",
    "        # if len(predictions) == 0:\n",
    "            for event1, event2 in combinations(doc.ents, 2):\n",
    "            # entities are actually in order\n",
    "                for verb in verbs:\n",
    "                    is_backwards, keyword = self.backwards(verb, doc)\n",
    "                    if is_backwards:\n",
    "                        if self.token_in_between_events(keyword, event1, event2):\n",
    "                            predictions.append(((event2.start, event2.end), (event1.start, event1.end)))\n",
    "                    else:\n",
    "                        if self.token_in_between_events(verb, event1, event2):\n",
    "                            predictions.append(((event1.start, event1.end), (event2.start, event2.end)))\n",
    "\n",
    "        if len(predictions) == 0:\n",
    "            for event1, event2 in combinations(doc.ents, 2):\n",
    "                predictions.append(((event1.start, event1.end), (event2.start, event2.end)))\n",
    "\n",
    "        predictions = set(predictions)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pipeline:\n",
    "    def __init__(self, train_sentences, train_labels) :\n",
    "        self.train_sentences = train_sentences\n",
    "        self.train_labels = train_labels\n",
    "        \n",
    "        self.is_causal_predictor = is_causal_predictor(train_sentences,train_labels)\n",
    "        self.span_extractor = None\n",
    "        self.relation_classificator = relation_classificator()\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        is_causal = self.is_causal_predictor.predict_causality(sentence)\n",
    "        if is_causal:\n",
    "            spans = self.span_extractor.predict()\n",
    "            prediction = self.relation_classificator.predict(sentence, spans)\n",
    "        else:\n",
    "            prediction = []\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['associate',\n",
       " 'lead',\n",
       " 'cause',\n",
       " 'include',\n",
       " 'increase',\n",
       " 'agep',\n",
       " 'hht',\n",
       " 'result',\n",
       " 'induce',\n",
       " 'commonly',\n",
       " 'relate',\n",
       " 'occur',\n",
       " 'trigger',\n",
       " 'Haiti',\n",
       " '1',\n",
       " 'uroporphyrinogen',\n",
       " 'approximately',\n",
       " 'develop',\n",
       " 'lobe',\n",
       " 'relatively',\n",
       " 'particularly',\n",
       " '10',\n",
       " 'underlie',\n",
       " 'call',\n",
       " 'AVMs']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(train_sentences, train_labels)\n",
    "pipe.is_causal_predictor.causal_cues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
