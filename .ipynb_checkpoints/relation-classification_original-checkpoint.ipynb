{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install iobes\n",
    "#!pip install seqeval\n",
    "#!pip install sklearn_crfsuite\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Doc, SpanGroup\n",
    "from spacy.matcher import Matcher\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "from tqdm import autonotebook as tqdm\n",
    "from spacy.training import biluo_tags_to_spans\n",
    "import iobes\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from seqeval import scheme\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"./data/teaching-dataset\")\n",
    "with (data_dir / \"relation_classification_text_train.zip\").open(\"rb\") as file:\n",
    "    zip_file = ZipFile(file)\n",
    "    with zip_file.open(\"input.txt\") as f:\n",
    "        sentences = [\n",
    "            sentence.split(\"\\n\") for sentence in f.read().decode(\"utf-8\").split(\"\\n\\n\")\n",
    "        ]\n",
    "with (data_dir / \"relation_classification_references_train.zip\").open(\"rb\") as file:\n",
    "    zip_file = ZipFile(file)\n",
    "    with zip_file.open(\"references.txt\") as f:\n",
    "        labels = []\n",
    "        for line in f.read().decode(\"utf-8\").split(\"\\n\"):\n",
    "            relations = []\n",
    "            for relation in re.finditer(r\"\\(\\((\\d+),(\\d+)\\),\\((\\d+),(\\d+)\\)\\)\", line):\n",
    "                relation = (\n",
    "                    (int(relation.group(1)), int(relation.group(2))),\n",
    "                    (int(relation.group(3)), int(relation.group(4))),\n",
    "                )\n",
    "                relations.append(relation)\n",
    "            labels.append(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence(sentence):\n",
    "    words = []\n",
    "    tags = []\n",
    "    for item in sentence:\n",
    "        word, tag = item.split(\" \")\n",
    "        words.append(word)\n",
    "        tags.append(tag)\n",
    "    doc = Doc(nlp.vocab, words=words)\n",
    "    doc = nlp(doc)\n",
    "    tags = iobes.bio_to_bilou(tags)\n",
    "    doc.ents = biluo_tags_to_spans(doc, tags)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According O\n",
      "two O\n",
      "different O\n",
      "studies O\n",
      "it O\n",
      "seems O\n",
      "plausible O\n",
      "that O\n",
      "the O\n",
      "Pohang B-EVENT\n",
      "earthquake I-EVENT\n",
      "was O\n",
      "induced O\n",
      "by O\n",
      "EGS B-EVENT\n",
      "operations I-EVENT\n",
      ". O\n"
     ]
    }
   ],
   "source": [
    "for word in sentences[0]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for word in sentences[0]:\n",
    "#    print(word.split())\n",
    "#    print(word.split()[0])\n",
    "#    print(word.split()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((14, 16), (9, 11))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "print(labels[1][0][0][0])\n",
    "print(labels[1][0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Pohang earthquake, EGS operations)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = parse_sentence(sentences[0])\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Sentence # ID in Sentence       Word    POS     DEP      Tag  \\\n",
      "0        Sentence: 0              0  According   VERB    prep        O   \n",
      "1        Sentence: 0              1        two    NUM  nummod        O   \n",
      "2        Sentence: 0              2  different    ADJ    amod        O   \n",
      "3        Sentence: 0              3    studies   NOUN    pobj        O   \n",
      "4        Sentence: 0              4         it   PRON   nsubj        O   \n",
      "...              ...            ...        ...    ...     ...      ...   \n",
      "13425  Sentence: 467             28         of    ADP    prep        O   \n",
      "13426  Sentence: 467             29      death   NOUN    pobj  B-EVENT   \n",
      "13427  Sentence: 467             30        for    ADP    prep        O   \n",
      "13428  Sentence: 467             31   Haitians  PROPN    pobj        O   \n",
      "13429  Sentence: 467             32          .  PUNCT   punct        O   \n",
      "\n",
      "                                                    Ents CustomClass  \n",
      "0                    (Pohang earthquake, EGS operations)      1 -> 0  \n",
      "1                    (Pohang earthquake, EGS operations)      1 -> 0  \n",
      "2                    (Pohang earthquake, EGS operations)      1 -> 0  \n",
      "3                    (Pohang earthquake, EGS operations)      1 -> 0  \n",
      "4                    (Pohang earthquake, EGS operations)      1 -> 0  \n",
      "...                                                  ...         ...  \n",
      "13425  (High infection rates for diseases, respirator...      1 -> 0  \n",
      "13426  (High infection rates for diseases, respirator...      1 -> 0  \n",
      "13427  (High infection rates for diseases, respirator...      1 -> 0  \n",
      "13428  (High infection rates for diseases, respirator...      1 -> 0  \n",
      "13429  (High infection rates for diseases, respirator...      1 -> 0  \n",
      "\n",
      "[13430 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "sentence_number=[]\n",
    "words=[]\n",
    "tags=[]\n",
    "dependencies=[]\n",
    "events=[]\n",
    "ents=[]\n",
    "labels1=[]\n",
    "ID=[]\n",
    "customclass=[]\n",
    "for i in range(len(sentences)):\n",
    "    doc = parse_sentence(sentences[i])\n",
    "    for token in doc:\n",
    "        sentence_number.append(f'Sentence: {i}')\n",
    "        #words.append(str(token).split()[0])\n",
    "        words.append(str(token))\n",
    "        tags.append(token.pos_)\n",
    "        dependencies.append(token.dep_)\n",
    "        ents.append(str(doc.ents))\n",
    "        labels1.append(str(labels[i]))\n",
    "        ID.append(str(token.i))\n",
    "        if labels[i][0][0][0] < labels[0][0][1][0]:\n",
    "            customclass.append('0 -> 1')\n",
    "        elif labels[i][0][0][0] >= labels[0][0][1][0]:\n",
    "            customclass.append('1 -> 0')\n",
    "        \n",
    "    for word in sentences[i]:\n",
    "        events.append(word.split()[1])\n",
    "\n",
    "\n",
    "#train_data = {'Sentence #': sentence_number, 'ID in Sentence': ID, 'Word': words, 'POS': tags, 'DEP': dependencies, 'Tag': events, 'Labels': labels1, 'Ents': ents, 'CustomClass': customclass}\n",
    "train_data = {'Sentence #': sentence_number, 'ID in Sentence': ID, 'Word': words, 'POS': tags, 'DEP': dependencies, 'Tag': events, 'Ents': ents, 'CustomClass': customclass}\n",
    "df = pd.DataFrame(data=train_data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1 -> 0\n",
      "1     1 -> 0\n",
      "2     1 -> 0\n",
      "3     1 -> 0\n",
      "4     1 -> 0\n",
      "       ...  \n",
      "95    0 -> 1\n",
      "96    0 -> 1\n",
      "97    0 -> 1\n",
      "98    0 -> 1\n",
      "99    0 -> 1\n",
      "Name: CustomClass, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.CustomClass.iloc[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10744, 4590), (10744,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n",
    "\n",
    "df = df.fillna(method='ffill')\n",
    "df['Sentence #'].nunique(), df.Word.nunique(), df.Tag.nunique()\n",
    "\n",
    "#df.groupby('Tag').size().reset_index(name='counts')\n",
    "df.groupby('CustomClass').size().reset_index(name='counts')\n",
    "\n",
    "#X = df.drop('Tag', axis=1)\n",
    "X = df.drop('CustomClass', axis=1)\n",
    "v = DictVectorizer(sparse=False)\n",
    "X = v.fit_transform(X.to_dict('records'))\n",
    "#y = df.Tag.values\n",
    "y = df.CustomClass.values\n",
    "classes = np.unique(y)\n",
    "classes = classes.tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 69.46, NNZs: 1390, Bias: -1.000000, T: 10744, Avg. loss: 0.114576\n",
      "Total training time: 0.04 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Perceptron(max_iter=5, n_jobs=-1, verbose=10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per = Perceptron(verbose=10, n_jobs=-1, max_iter=5)\n",
    "per.partial_fit(X_train, y_train, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0 -> 1', '1 -> 0']\n"
     ]
    }
   ],
   "source": [
    "new_classes = classes.copy()\n",
    "print(new_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      0 -> 1       1.00      1.00      1.00      1627\n",
      "      1 -> 0       1.00      0.99      1.00      1059\n",
      "\n",
      "    accuracy                           1.00      2686\n",
      "   macro avg       1.00      1.00      1.00      2686\n",
      "weighted avg       1.00      1.00      1.00      2686\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred=per.predict(X_test), y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_active_passive(doc):\n",
    "    # https://stackoverflow.com/questions/74528441/detect-passive-or-active-sentence-from-text\n",
    "    passive_rules = [\n",
    "        [\n",
    "            {\"DEP\": \"nsubjpass\"},\n",
    "            {\"DEP\": \"aux\", \"OP\": \"*\"},\n",
    "            {\"DEP\": \"auxpass\"},\n",
    "            {\"TAG\": \"VBN\"},\n",
    "        ],\n",
    "        [\n",
    "            {\"DEP\": \"nsubjpass\"},\n",
    "            {\"DEP\": \"aux\", \"OP\": \"*\"},\n",
    "            {\"DEP\": \"auxpass\"},\n",
    "            {\"TAG\": \"VBZ\"},\n",
    "        ],\n",
    "        [\n",
    "            {\"DEP\": \"nsubjpass\"},\n",
    "            {\"DEP\": \"aux\", \"OP\": \"*\"},\n",
    "            {\"DEP\": \"auxpass\"},\n",
    "            {\"TAG\": \"RB\"},\n",
    "            {\"TAG\": \"VBN\"},\n",
    "        ],\n",
    "    ]\n",
    "    # Create pattern to match active voice use\n",
    "    active_rules = [\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"VBD\", \"DEP\": \"ROOT\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"VBP\"}, {\"TAG\": \"VBG\", \"OP\": \"!\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"DEP\": \"aux\", \"OP\": \"*\"}, {\"TAG\": \"VB\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"DEP\": \"aux\", \"OP\": \"*\"}, {\"TAG\": \"VBG\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"RB\", \"OP\": \"*\"}, {\"TAG\": \"VBG\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"RB\", \"OP\": \"*\"}, {\"TAG\": \"VBZ\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"RB\", \"OP\": \"+\"}, {\"TAG\": \"VBD\"}],\n",
    "    ]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Passive\", passive_rules)\n",
    "    matcher.add(\"Active\", active_rules)\n",
    "    matches = matcher(doc)\n",
    "    matches = [\n",
    "        (nlp.vocab.strings[match_id], doc[start:end])\n",
    "        for match_id, start, end in matches\n",
    "    ]\n",
    "    return matches\n",
    "\n",
    "\n",
    "def predict(sentence):\n",
    "    doc = parse_sentence(sentence)\n",
    "    matches = extract_active_passive(doc)\n",
    "    predictions = []\n",
    "    for ent_1, ent_2 in combinations(doc.ents, 2):\n",
    "        for match_type, match_span in matches:\n",
    "            if SpanGroup(doc, spans=[ent_1, ent_2, match_span]).has_overlap:\n",
    "                match_active = match_type == \"Active\"\n",
    "                if match_active:\n",
    "                    predictions.append(\n",
    "                        ((ent_1.start, ent_1.end), (ent_2.start, ent_2.end))\n",
    "                    )\n",
    "                    break\n",
    "                else:\n",
    "                    predictions.append(\n",
    "                        ((ent_2.start, ent_2.end), (ent_1.start, ent_1.end))\n",
    "                    )\n",
    "                    break\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According two different studies it seems plausible that the Pohang earthquake was induced by EGS operations . \n",
      "\tActive: it seems\n",
      "\tPassive: earthquake was induced\n"
     ]
    }
   ],
   "source": [
    "doc = parse_sentence(sentences[0])\n",
    "matches = extract_active_passive(doc)\n",
    "print(doc)\n",
    "for match_type, match_span in matches:\n",
    "    print(\"\\t{}: {}\".format(match_type, match_span.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serum sickness can be developed as a result of exposure to antibodies derived from animals . \n",
      "Ground truth:\n",
      "\tSerum sickness -> exposure to antibodies derived from animals\n",
      "Predictions:\n",
      "\texposure to antibodies derived from animals -> Serum sickness\n"
     ]
    }
   ],
   "source": [
    "idx = 5\n",
    "doc = parse_sentence(sentences[idx])\n",
    "pred = predict(sentences[idx])\n",
    "print(doc)\n",
    "print(\"Ground truth:\")\n",
    "for cause, effect in labels[idx]:\n",
    "    print(\"\\t{} -> {}\".format(doc[cause[0]:cause[1]], doc[effect[0]:effect[1]]))\n",
    "print(\"Predictions:\")\n",
    "for cause, effect in pred:\n",
    "    print(\"\\t{} -> {}\".format(doc[cause[0]:cause[1]], doc[effect[0]:effect[1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07798b888ca848168c98bb8af1c9b7d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = []\n",
    "for sentence in tqdm.tqdm(sentences):\n",
    "    predictions.append(predict(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Precision: 0.47\n",
      "Micro Recall: 0.22\n",
      "Micro F1: 0.30\n",
      "Macro Precision: 0.26\n",
      "Macro Recall: 0.27\n",
      "Macro F1: 0.26\n"
     ]
    }
   ],
   "source": [
    "def evaluate(predictions, references, micro_avg=True):\n",
    "    tp = []\n",
    "    fp = []\n",
    "    fn = []\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        tp.append(len(set(prediction) & set(reference)))\n",
    "        fp.append(len(set(prediction) - set(reference)))\n",
    "        fn.append(len(set(reference) - set(prediction)))\n",
    "    if micro_avg:\n",
    "        tp = [sum(tp)]\n",
    "        fp = [sum(fp)]\n",
    "        fn = [sum(fn)]\n",
    "    precision = [0 if tp[i] == 0 else tp[i] / (tp[i] + fp[i]) for i in range(len(tp))]\n",
    "    recall = [0 if tp[i] == 0 else tp[i] / (tp[i] + fn[i]) for i in range(len(tp))]\n",
    "    f1 = [\n",
    "        0\n",
    "        if precision[i] * recall[i] == 0\n",
    "        else 2 * precision[i] * recall[i] / (precision[i] + recall[i])\n",
    "        for i in range(len(tp))\n",
    "    ]\n",
    "    precision = sum(precision) / len(precision)\n",
    "    recall = sum(recall) / len(recall)\n",
    "    f1 = sum(f1) / len(f1)\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "micro_precision, micro_recall, micro_f1 = evaluate(predictions, labels, True)\n",
    "macro_precision, macro_recall, macro_f1 = evaluate(predictions, labels, False)\n",
    "\n",
    "print(\"Micro Precision: {:.2f}\".format(micro_precision))\n",
    "print(\"Micro Recall: {:.2f}\".format(micro_recall))\n",
    "print(\"Micro F1: {:.2f}\".format(micro_f1))\n",
    "print(\"Macro Precision: {:.2f}\".format(macro_precision))\n",
    "print(\"Macro Recall: {:.2f}\".format(macro_recall))\n",
    "print(\"Macro F1: {:.2f}\".format(macro_f1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
