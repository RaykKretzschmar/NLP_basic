{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install iobes\n",
    "#!pip install seqeval\n",
    "#!pip install sklearn_crfsuite \n",
    "#!conda install spacy\n",
    "#!pip uninstall numpy\n",
    "#!pip install numpy\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Doc, SpanGroup\n",
    "from spacy.matcher import Matcher\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "#from tqdm import autonotebook as tqdm\n",
    "from tqdm import tqdm\n",
    "from spacy.training import biluo_tags_to_spans\n",
    "import iobes\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from seqeval import scheme\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"./data/teaching-dataset\")\n",
    "with (data_dir / \"relation_classification_text_train.zip\").open(\"rb\") as file:\n",
    "    zip_file = ZipFile(file)\n",
    "    with zip_file.open(\"input.txt\") as f:\n",
    "        sentences = [\n",
    "            sentence.split(\"\\n\") for sentence in f.read().decode(\"utf-8\").split(\"\\n\\n\")\n",
    "        ]\n",
    "with (data_dir / \"relation_classification_references_train.zip\").open(\"rb\") as file:\n",
    "    zip_file = ZipFile(file)\n",
    "    with zip_file.open(\"references.txt\") as f:\n",
    "        labels = []\n",
    "        for line in f.read().decode(\"utf-8\").split(\"\\n\"):\n",
    "            relations = []\n",
    "            for relation in re.finditer(r\"\\(\\((\\d+),(\\d+)\\),\\((\\d+),(\\d+)\\)\\)\", line):\n",
    "                relation = (\n",
    "                    (int(relation.group(1)), int(relation.group(2))),\n",
    "                    (int(relation.group(3)), int(relation.group(4))),\n",
    "                )\n",
    "                relations.append(relation)\n",
    "            labels.append(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence(sentence):\n",
    "    words = []\n",
    "    tags = []\n",
    "    for item in sentence:\n",
    "        word, tag = item.split(\" \")\n",
    "        words.append(word)\n",
    "        tags.append(tag)\n",
    "    doc = Doc(nlp.vocab, words=words)\n",
    "    doc = nlp(doc)\n",
    "    tags = iobes.bio_to_bilou(tags)\n",
    "    doc.ents = biluo_tags_to_spans(doc, tags)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['According', 'two', 'different', 'studies', 'it', 'seems', 'plausible', 'that', 'the', 'Pohang', 'earthquake', 'was', 'induced', 'by', 'EGS', 'operations', '.']\n",
      "[['According', 'two', 'different', 'studies', 'it', 'seems', 'plausible', 'that', 'the', 'Pohang', 'earthquake', 'was', 'induced', 'by', 'EGS', 'operations', '.'], 'test']\n"
     ]
    }
   ],
   "source": [
    "wordlist = []\n",
    "biggerlist = []\n",
    "for word in sentences[0]:\n",
    "    #print(word)\n",
    "    wordlist.append(str(word.split()[0]))\n",
    "print(wordlist)\n",
    "biggerlist.append(wordlist)\n",
    "biggerlist.append('test')\n",
    "print(biggerlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = []\n",
    "taglist = []\n",
    "biggertaglist = []\n",
    "biggerlist = []\n",
    "for i in range(len(sentences)):\n",
    "    for word in sentences[i]:\n",
    "        wordlist.append(str(word.split()[0]))\n",
    "        taglist.append(str(word.split()[1]))\n",
    "\n",
    "        \n",
    "    biggerlist.append(wordlist)\n",
    "    biggertaglist.append(taglist)\n",
    "    \n",
    "    wordlist = []\n",
    "    taglist = []\n",
    "#print(biggerlist)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for word in sentences[0]:\n",
    "#    print(word.split())\n",
    "#    print(word.split()[0])\n",
    "#    print(word.split()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((14, 16), (9, 11))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(14, 16)\n",
      "14\n",
      "16\n",
      "(9, 11)\n",
      "9\n",
      "11\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "len(labels[467])\n",
    "len(sentences[0])\n",
    "#helper = np.zeros((len(sentences[0]), len(sentences[0])))\n",
    "helper = np.zeros(len(sentences[0]))\n",
    "print(helper)\n",
    "\n",
    "for entry in labels[0][0]:\n",
    "    print(entry)\n",
    "    for numbers in entry:\n",
    "        print(numbers)\n",
    "        helper[numbers-1] = 1\n",
    "print(helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "print(labels[1][0][0][0])\n",
    "print(labels[1][0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Pohang earthquake, EGS operations)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = parse_sentence(sentences[0])\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sentence #                                           Wordlist  \\\n",
      "0            0  [According, two, different, studies, it, seems...   \n",
      "1            1  [Signs, and, symptoms, include, :, Dyspnea, (,...   \n",
      "2            2  [Pneumatic, drilling, in, mines, and, less, co...   \n",
      "3            3  [Interleukin-1, inhibitors, ,, such, as, canak...   \n",
      "4            4  [Two, of, the, reviews, postulate, that, exerc...   \n",
      "..         ...                                                ...   \n",
      "463        463  [Dental, fluorosis, (, also, termed, mottled, ...   \n",
      "464        464  [The, detergent, causes, the, vessel, to, coll...   \n",
      "465        465  [Large, segmental, hemangiomas, of, the, head,...   \n",
      "466        466  [Following, a, strain, or, partial, rupture, o...   \n",
      "467        467  [High, infection, rates, for, diseases, such, ...   \n",
      "\n",
      "                                               Taglist  \\\n",
      "0    [O, O, O, O, O, O, O, O, O, B-EVENT, I-EVENT, ...   \n",
      "1    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
      "2    [B-EVENT, I-EVENT, I-EVENT, I-EVENT, O, O, O, ...   \n",
      "3    [B-EVENT, I-EVENT, O, O, O, B-EVENT, O, O, O, ...   \n",
      "4    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
      "..                                                 ...   \n",
      "463  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
      "464  [O, B-EVENT, O, O, B-EVENT, I-EVENT, I-EVENT, ...   \n",
      "465  [B-EVENT, I-EVENT, I-EVENT, I-EVENT, I-EVENT, ...   \n",
      "466  [O, O, O, O, O, O, O, O, O, O, O, O, O, B-EVEN...   \n",
      "467  [B-EVENT, I-EVENT, I-EVENT, I-EVENT, I-EVENT, ...   \n",
      "\n",
      "                                                   POS  \\\n",
      "0    [VERB, NUM, ADJ, NOUN, PRON, VERB, ADJ, SCONJ,...   \n",
      "1    [NOUN, CCONJ, NOUN, VERB, PUNCT, PROPN, PUNCT,...   \n",
      "2    [ADJ, NOUN, ADP, NOUN, CCONJ, ADV, ADV, PUNCT,...   \n",
      "3    [PROPN, NOUN, PUNCT, ADJ, ADP, NOUN, PUNCT, VE...   \n",
      "4    [NUM, ADP, DET, NOUN, VERB, SCONJ, NOUN, AUX, ...   \n",
      "..                                                 ...   \n",
      "463  [ADJ, NOUN, PUNCT, ADV, VERB, PROPN, PROPN, PU...   \n",
      "464  [DET, NOUN, VERB, DET, NOUN, PART, VERB, CCONJ...   \n",
      "465  [ADJ, ADJ, NOUN, ADP, DET, NOUN, CCONJ, NOUN, ...   \n",
      "466  [VERB, DET, NOUN, CCONJ, ADJ, NOUN, ADP, ADJ, ...   \n",
      "467  [ADJ, NOUN, NOUN, ADP, NOUN, ADJ, ADP, ADJ, NO...   \n",
      "\n",
      "                                                   DEP customclass  \n",
      "0    [prep, nummod, amod, pobj, nsubj, ROOT, oprd, ...           1  \n",
      "1    [nsubj, cc, conj, ROOT, punct, nsubj, punct, a...           1  \n",
      "2    [amod, nsubj, prep, pobj, cc, advmod, conj, pu...           0  \n",
      "3    [compound, nsubj, punct, amod, prep, pobj, pun...           0  \n",
      "4    [nsubj, prep, det, pobj, ROOT, mark, nsubj, cc...           1  \n",
      "..                                                 ...         ...  \n",
      "463  [amod, nsubj, punct, advmod, acl, compound, op...           1  \n",
      "464  [det, nsubj, ROOT, det, nsubj, aux, ccomp, cc,...           0  \n",
      "465  [amod, amod, nsubjpass, prep, det, pobj, cc, c...           1  \n",
      "466  [prep, det, pobj, cc, amod, conj, prep, amod, ...           1  \n",
      "467  [amod, compound, nsubj, prep, pobj, amod, prep...           1  \n",
      "\n",
      "[468 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#new df with 1 line per sentence \n",
    "sentence_number=[]\n",
    "wordlist = []\n",
    "taglist = []\n",
    "POSlist = []\n",
    "DEPlist = []\n",
    "customclass = []\n",
    "\n",
    "biggerlist = []\n",
    "biggertaglist = []\n",
    "biggerPOSlist = []\n",
    "biggerDEPlist = []\n",
    "for i in range(len(sentences)):\n",
    "    if labels[i][0][0][0] < labels[0][0][1][0]:\n",
    "        customclass.append('0')\n",
    "    elif labels[i][0][0][0] >= labels[0][0][1][0]:\n",
    "        customclass.append('1')\n",
    "for i in range(len(sentences)):\n",
    "    sentence_number.append(str(i))\n",
    "    \n",
    "    for word in sentences[i]:\n",
    "        wordlist.append(str(word.split()[0]))\n",
    "        taglist.append(str(word.split()[1]))\n",
    "\n",
    "    doc = parse_sentence(sentences[i])\n",
    "    for token in doc:\n",
    "        POSlist.append(token.pos_)\n",
    "        DEPlist.append(token.dep_)\n",
    "            \n",
    "\n",
    "    biggerlist.append(wordlist)\n",
    "    biggertaglist.append(taglist)\n",
    "    biggerPOSlist.append(POSlist)\n",
    "    biggerDEPlist.append(DEPlist)\n",
    "    \n",
    "    wordlist = []\n",
    "    taglist = []\n",
    "    POSlist = []\n",
    "    DEPlist = []\n",
    "\n",
    "train_data = {'Sentence #': sentence_number, 'Wordlist': biggerlist, 'Taglist': biggertaglist, 'POS': biggerPOSlist, 'DEP': biggerDEPlist, 'customclass': customclass,}\n",
    "#train_data = {'Sentence #': sentence_number, 'Wordlist': str(biggerlist), 'Taglist': str(biggertaglist), 'POS': str(biggerPOSlist), 'DEP': str(biggerDEPlist), 'customclass': customclass,}\n",
    "df = pd.DataFrame(data=train_data)\n",
    "print(df)\n",
    "#df.to_dict()\n",
    "#print(df)\n",
    "#df.to_dict('records')\n",
    "#print(df)\n",
    "#df.to_dict('split')\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  customclass  counts\n",
      "0           0     314\n",
      "1           1     154\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "[1. 1. 0. ... 0. 0. 0.]\n",
      "[1. 1. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((374, 4015), (374,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n",
    "\n",
    "df = df.fillna(method='ffill')\n",
    "#df['Sentence #'].nunique(), df.Wordlist.nunique(), df.Taglist.nunique(), df.customclass.nunique()\n",
    "#print(df['Sentence #'].nunique())\n",
    "#print(df.Wordlist.nunique())\n",
    "#print(df.Taglist.nunique())\n",
    "#print(df.CustomClass.nunique())\n",
    "\n",
    "#df.groupby('Tag').size().reset_index(name='counts')\n",
    "#print(df.groupby('Tag').size().reset_index(name='counts'))\n",
    "\n",
    "df.groupby('customclass').size().reset_index(name='counts')\n",
    "print(df.groupby('customclass').size().reset_index(name='counts'))\n",
    "\n",
    "#X = df.drop('Tag', axis=1)\n",
    "X = df.drop('customclass', axis=1)\n",
    "#print(X)\n",
    "\n",
    "v = DictVectorizer(sparse=False)\n",
    "\n",
    "X = v.fit_transform(X.to_dict('records'))\n",
    "\n",
    "for i in range(10):\n",
    "    print(X[i])\n",
    "    \n",
    "#y = df.Tag.values\n",
    "y = df.customclass.values\n",
    "classes = np.unique(y)\n",
    "classes = classes.tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 116.55, NNZs: 1881, Bias: -22.000000, T: 374, Avg. loss: 223.491979\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Perceptron(max_iter=5, n_jobs=-1, verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Perceptron</label><div class=\"sk-toggleable__content\"><pre>Perceptron(max_iter=5, n_jobs=-1, verbose=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Perceptron(max_iter=5, n_jobs=-1, verbose=10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per = Perceptron(verbose=10, n_jobs=-1, max_iter=5)\n",
    "per.partial_fit(X_train, y_train, classes)\n",
    "#per.fit(X_train, y_train, classes, coef_init = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#per.partial_fit(X_train, y_train, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1']\n"
     ]
    }
   ],
   "source": [
    "new_classes = classes.copy()\n",
    "print(new_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.66      0.70        64\n",
      "           1       0.42      0.53      0.47        30\n",
      "\n",
      "    accuracy                           0.62        94\n",
      "   macro avg       0.59      0.59      0.59        94\n",
      "weighted avg       0.65      0.62      0.63        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred=per.predict(X_test), y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1' '1' '0' '1' '1' '1' '0' '1' '0' '0' '1' '0' '1' '0' '1' '1' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '1' '1' '1' '0'\n",
      " '1' '1' '0' '1' '0' '1' '0' '1' '0' '1' '0' '0' '0' '1' '0' '0' '0' '0'\n",
      " '1' '0' '1' '1' '0' '1' '0' '0' '0' '0' '1' '0' '1' '1' '0' '0' '1' '0'\n",
      " '1' '1' '1' '0' '0' '0' '0' '1' '1' '1' '0' '0' '1' '0' '1' '0' '0' '0'\n",
      " '0' '0' '0' '1']\n",
      "['0' '0' '1' '0' '1' '0' '0' '1' '1' '0' '0' '1' '1' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '1' '0' '0' '1' '0' '0' '1' '1'\n",
      " '0' '0' '0' '0' '0' '0' '0' '1' '1' '1' '1' '0' '0' '1' '0' '0' '1' '0'\n",
      " '0' '0' '1' '0' '0' '1' '0' '0' '0' '0' '0' '1' '1' '0' '1' '0' '1' '0'\n",
      " '1' '0' '0' '0' '0' '0' '1' '1' '0' '1' '0' '1' '1' '1' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "print(per.predict(X_test))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_active_passive(doc):\n",
    "    # https://stackoverflow.com/questions/74528441/detect-passive-or-active-sentence-from-text\n",
    "    passive_rules = [\n",
    "        [\n",
    "            {\"DEP\": \"nsubjpass\"},\n",
    "            {\"DEP\": \"aux\", \"OP\": \"*\"},\n",
    "            {\"DEP\": \"auxpass\"},\n",
    "            {\"TAG\": \"VBN\"},\n",
    "        ],\n",
    "        [\n",
    "            {\"DEP\": \"nsubjpass\"},\n",
    "            {\"DEP\": \"aux\", \"OP\": \"*\"},\n",
    "            {\"DEP\": \"auxpass\"},\n",
    "            {\"TAG\": \"VBZ\"},\n",
    "        ],\n",
    "        [\n",
    "            {\"DEP\": \"nsubjpass\"},\n",
    "            {\"DEP\": \"aux\", \"OP\": \"*\"},\n",
    "            {\"DEP\": \"auxpass\"},\n",
    "            {\"TAG\": \"RB\"},\n",
    "            {\"TAG\": \"VBN\"},\n",
    "        ],\n",
    "    ]\n",
    "    # Create pattern to match active voice use\n",
    "    active_rules = [\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"VBD\", \"DEP\": \"ROOT\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"VBP\"}, {\"TAG\": \"VBG\", \"OP\": \"!\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"DEP\": \"aux\", \"OP\": \"*\"}, {\"TAG\": \"VB\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"DEP\": \"aux\", \"OP\": \"*\"}, {\"TAG\": \"VBG\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"RB\", \"OP\": \"*\"}, {\"TAG\": \"VBG\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"RB\", \"OP\": \"*\"}, {\"TAG\": \"VBZ\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"RB\", \"OP\": \"+\"}, {\"TAG\": \"VBD\"}],\n",
    "    ]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Passive\", passive_rules)\n",
    "    matcher.add(\"Active\", active_rules)\n",
    "    matches = matcher(doc)\n",
    "    matches = [\n",
    "        (nlp.vocab.strings[match_id], doc[start:end])\n",
    "        for match_id, start, end in matches\n",
    "    ]\n",
    "    return matches\n",
    "\n",
    "\n",
    "def predict(sentence):\n",
    "    doc = parse_sentence(sentence)\n",
    "    matches = extract_active_passive(doc)\n",
    "    predictions = []\n",
    "    for ent_1, ent_2 in combinations(doc.ents, 2):\n",
    "        for match_type, match_span in matches:\n",
    "            if SpanGroup(doc, spans=[ent_1, ent_2, match_span]).has_overlap:\n",
    "                match_active = match_type == \"Active\"\n",
    "                if match_active:\n",
    "                    predictions.append(\n",
    "                        ((ent_1.start, ent_1.end), (ent_2.start, ent_2.end))\n",
    "                    )\n",
    "                    break\n",
    "                else:\n",
    "                    predictions.append(\n",
    "                        ((ent_2.start, ent_2.end), (ent_1.start, ent_1.end))\n",
    "                    )\n",
    "                    break\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According two different studies it seems plausible that the Pohang earthquake was induced by EGS operations . \n",
      "\tActive: it seems\n",
      "\tPassive: earthquake was induced\n"
     ]
    }
   ],
   "source": [
    "doc = parse_sentence(sentences[0])\n",
    "matches = extract_active_passive(doc)\n",
    "print(doc)\n",
    "for match_type, match_span in matches:\n",
    "    print(\"\\t{}: {}\".format(match_type, match_span.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serum sickness can be developed as a result of exposure to antibodies derived from animals . \n",
      "Ground truth:\n",
      "\tSerum sickness -> exposure to antibodies derived from animals\n",
      "Predictions:\n",
      "\texposure to antibodies derived from animals -> Serum sickness\n"
     ]
    }
   ],
   "source": [
    "idx = 5\n",
    "doc = parse_sentence(sentences[idx])\n",
    "pred = predict(sentences[idx])\n",
    "print(doc)\n",
    "print(\"Ground truth:\")\n",
    "for cause, effect in labels[idx]:\n",
    "    print(\"\\t{} -> {}\".format(doc[cause[0]:cause[1]], doc[effect[0]:effect[1]]))\n",
    "print(\"Predictions:\")\n",
    "for cause, effect in pred:\n",
    "    print(\"\\t{} -> {}\".format(doc[cause[0]:cause[1]], doc[effect[0]:effect[1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 468/468 [00:03<00:00, 136.46it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "#for sentence in tqdm.tqdm(sentences):\n",
    "for sentence in tqdm(sentences):\n",
    "    predictions.append(predict(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Precision: 0.47\n",
      "Micro Recall: 0.22\n",
      "Micro F1: 0.30\n",
      "Macro Precision: 0.26\n",
      "Macro Recall: 0.27\n",
      "Macro F1: 0.26\n"
     ]
    }
   ],
   "source": [
    "def evaluate(predictions, references, micro_avg=True):\n",
    "    tp = []\n",
    "    fp = []\n",
    "    fn = []\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        tp.append(len(set(prediction) & set(reference)))\n",
    "        fp.append(len(set(prediction) - set(reference)))\n",
    "        fn.append(len(set(reference) - set(prediction)))\n",
    "    if micro_avg:\n",
    "        tp = [sum(tp)]\n",
    "        fp = [sum(fp)]\n",
    "        fn = [sum(fn)]\n",
    "    precision = [0 if tp[i] == 0 else tp[i] / (tp[i] + fp[i]) for i in range(len(tp))]\n",
    "    recall = [0 if tp[i] == 0 else tp[i] / (tp[i] + fn[i]) for i in range(len(tp))]\n",
    "    f1 = [\n",
    "        0\n",
    "        if precision[i] * recall[i] == 0\n",
    "        else 2 * precision[i] * recall[i] / (precision[i] + recall[i])\n",
    "        for i in range(len(tp))\n",
    "    ]\n",
    "    precision = sum(precision) / len(precision)\n",
    "    recall = sum(recall) / len(recall)\n",
    "    f1 = sum(f1) / len(f1)\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "micro_precision, micro_recall, micro_f1 = evaluate(predictions, labels, True)\n",
    "macro_precision, macro_recall, macro_f1 = evaluate(predictions, labels, False)\n",
    "\n",
    "print(\"Micro Precision: {:.2f}\".format(micro_precision))\n",
    "print(\"Micro Recall: {:.2f}\".format(micro_recall))\n",
    "print(\"Micro F1: {:.2f}\".format(micro_f1))\n",
    "print(\"Macro Precision: {:.2f}\".format(macro_precision))\n",
    "print(\"Macro Recall: {:.2f}\".format(macro_recall))\n",
    "print(\"Macro F1: {:.2f}\".format(macro_f1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
