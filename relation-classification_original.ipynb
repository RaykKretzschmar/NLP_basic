{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install iobes\n",
    "#!pip install seqeval\n",
    "#!pip install sklearn_crfsuite \n",
    "#!conda install spacy\n",
    "#!pip uninstall numpy\n",
    "#!pip install numpy\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Doc, SpanGroup\n",
    "from spacy.matcher import Matcher\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "#from tqdm import autonotebook as tqdm\n",
    "from tqdm import tqdm\n",
    "from spacy.training import biluo_tags_to_spans\n",
    "import iobes\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from seqeval import scheme\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"./data/teaching-dataset\")\n",
    "with (data_dir / \"relation_classification_text_train.zip\").open(\"rb\") as file:\n",
    "    zip_file = ZipFile(file)\n",
    "    with zip_file.open(\"input.txt\") as f:\n",
    "        sentences = [\n",
    "            sentence.split(\"\\n\") for sentence in f.read().decode(\"utf-8\").split(\"\\n\\n\")\n",
    "        ]\n",
    "with (data_dir / \"relation_classification_references_train.zip\").open(\"rb\") as file:\n",
    "    zip_file = ZipFile(file)\n",
    "    with zip_file.open(\"references.txt\") as f:\n",
    "        labels = []\n",
    "        for line in f.read().decode(\"utf-8\").split(\"\\n\"):\n",
    "            relations = []\n",
    "            for relation in re.finditer(r\"\\(\\((\\d+),(\\d+)\\),\\((\\d+),(\\d+)\\)\\)\", line):\n",
    "                relation = (\n",
    "                    (int(relation.group(1)), int(relation.group(2))),\n",
    "                    (int(relation.group(3)), int(relation.group(4))),\n",
    "                )\n",
    "                relations.append(relation)\n",
    "            labels.append(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence(sentence):\n",
    "    words = []\n",
    "    tags = []\n",
    "    for item in sentence:\n",
    "        word, tag = item.split(\" \")\n",
    "        words.append(word)\n",
    "        tags.append(tag)\n",
    "    doc = Doc(nlp.vocab, words=words)\n",
    "    doc = nlp(doc)\n",
    "    tags = iobes.bio_to_bilou(tags)\n",
    "    doc.ents = biluo_tags_to_spans(doc, tags)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['According', 'two', 'different', 'studies', 'it', 'seems', 'plausible', 'that', 'the', 'Pohang', 'earthquake', 'was', 'induced', 'by', 'EGS', 'operations', '.']\n",
      "[['According', 'two', 'different', 'studies', 'it', 'seems', 'plausible', 'that', 'the', 'Pohang', 'earthquake', 'was', 'induced', 'by', 'EGS', 'operations', '.'], 'test']\n"
     ]
    }
   ],
   "source": [
    "wordlist = []\n",
    "biggerlist = []\n",
    "for word in sentences[0]:\n",
    "    #print(word)\n",
    "    wordlist.append(str(word.split()[0]))\n",
    "print(wordlist)\n",
    "biggerlist.append(wordlist)\n",
    "biggerlist.append('test')\n",
    "print(biggerlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordlist = []\n",
    "#taglist = []\n",
    "#biggertaglist = []\n",
    "#biggerlist = []\n",
    "#for i in range(len(sentences)):\n",
    "#    for word in sentences[i]:\n",
    "#        wordlist.append(str(word.split()[0]))\n",
    "#        taglist.append(str(word.split()[1]))\n",
    "\n",
    "        \n",
    "#    biggerlist.append(wordlist)\n",
    "#    biggertaglist.append(taglist)\n",
    "    \n",
    "#    wordlist = []\n",
    "#    taglist = []\n",
    "#print(biggerlist)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for word in sentences[0]:\n",
    "#    print(word.split())\n",
    "#    print(word.split()[0])\n",
    "#    print(word.split()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 8, 9, 10, 12, 13, 18, 19, 20, 21, 22, 25, 27, 28, 30, 33, 34, 35, 36, 40, 43, 45, 47, 50, 51, 61, 64, 65, 67, 70, 72, 74, 77, 78, 79, 81, 82, 85, 86, 90, 91, 93, 95, 97, 99, 100, 102, 103, 104, 105, 106, 107, 108, 111, 112, 113, 114, 115, 118, 121, 123, 128, 130, 136, 138, 140, 145, 146, 149, 151, 155, 157, 159, 160, 170, 173, 174, 177, 179, 180, 182, 183, 185, 193, 194, 195, 197, 200, 202, 205, 209, 210, 211, 212, 220, 227, 228, 229, 230, 232, 233, 236, 238, 239, 240, 241, 246, 249, 252, 262, 264, 266, 268, 271, 275, 277, 279, 280, 284, 288, 294, 296, 301, 302, 303, 305, 307, 309, 312, 314, 316, 317, 320, 322, 323, 324, 325, 335, 338, 340, 343, 344, 346, 347, 349, 357, 359, 362, 369, 372, 373, 374, 382, 385, 388, 389, 390, 391, 392, 393, 394, 396, 397, 400, 401, 403, 407, 409, 410, 411, 413, 415, 422, 427, 433, 434, 437, 441, 442, 446, 448, 449, 450, 451, 452, 453, 454, 455, 456, 458, 460, 464, 466, 467]\n"
     ]
    }
   ],
   "source": [
    "#labels[0]\n",
    "# get list of all sentences with multiple relationships\n",
    "helperlist = []\n",
    "for i in range(len(sentences)):\n",
    "    if len(labels[i]) > 1:\n",
    "        helperlist.append(int(i))\n",
    "print(helperlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+000 1.40000000e+001]\n",
      " [0.00000000e+000 1.60000000e+001]\n",
      " [0.00000000e+000 9.00000000e+000]\n",
      " ...\n",
      " [5.06096000e-259 1.43711747e+246]\n",
      " [2.88970519e+159 4.17699191e+228]\n",
      " [1.46292066e+195 3.17091486e+180]]\n"
     ]
    }
   ],
   "source": [
    "# for each sentence, get list of event spans \n",
    "helperarray = np.empty([10000,2])\n",
    "counter2 = 0\n",
    "for i in range(len(sentences)):\n",
    "    #print(labels[i])\n",
    "    for entries in labels[i]:\n",
    "        #print(entries)\n",
    "        for numbers in entries:\n",
    "            #print(numbers)\n",
    "            for singlets in numbers:\n",
    "                #print(i, singlets)\n",
    "                helperarray[counter2] = [i, singlets]\n",
    "                counter2 += 1\n",
    "print(helperarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pneumatic B-EVENT\n",
      "drilling I-EVENT\n",
      "in I-EVENT\n",
      "mines I-EVENT\n",
      "and O\n",
      "less O\n",
      "commonly O\n",
      ", O\n",
      "mining B-EVENT\n",
      "using I-EVENT\n",
      "explosives I-EVENT\n",
      ", O\n",
      "would O\n",
      "raise O\n",
      "fine B-EVENT\n",
      "- I-EVENT\n",
      "ultra I-EVENT\n",
      "fine I-EVENT\n",
      "crystalline I-EVENT\n",
      "silica I-EVENT\n",
      "dust I-EVENT\n",
      "( O\n",
      "rock O\n",
      "dust O\n",
      ") O\n",
      ". O\n",
      "[0.0, 4.0, 14.0, 21.0, 8.0, 11.0, 14.0, 21.0]\n"
     ]
    }
   ],
   "source": [
    "# get list of event positions for each sentence\n",
    "linelist = []\n",
    "for lines in helperarray:\n",
    "    if lines[0] == 2.0:\n",
    "        #print(lines)\n",
    "        linelist.append(lines[1])\n",
    "for word in sentences[2]:\n",
    "    print(word)\n",
    "print(linelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 4.0, 14.0, 21.0]\n",
      "[8.0, 11.0, 14.0, 21.0]\n"
     ]
    }
   ],
   "source": [
    "# pop 4 instances from linelist\n",
    "splitSentenceList = []\n",
    "splitSentenceList.append(linelist.pop(0))\n",
    "splitSentenceList.append(linelist.pop(0))\n",
    "splitSentenceList.append(linelist.pop(0))\n",
    "splitSentenceList.append(linelist.pop(0))\n",
    "print(splitSentenceList)\n",
    "print(linelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pneumatic B-EVENT', 'drilling I-EVENT', 'in I-EVENT', 'mines I-EVENT', 'and O', 'less O', 'commonly O', ', O', 'mining B-EVENT', 'using I-EVENT', 'explosives I-EVENT', ', O', 'would O', 'raise O', 'fine B-EVENT', '- I-EVENT', 'ultra I-EVENT', 'fine I-EVENT', 'crystalline I-EVENT', 'silica I-EVENT', 'dust I-EVENT', '( O', 'rock O', 'dust O', ') O', '. O']\n",
      "['Pneumatic B-EVENT', 'drilling I-EVENT', 'in I-EVENT', 'mines I-EVENT', 'and O', 'less O', 'commonly O', ', O', 'mining B-EVENT', 'using I-EVENT', 'explosives I-EVENT', ', O', 'would O', 'raise O', 'fine B-EVENT', '- I-EVENT', 'ultra I-EVENT', 'fine I-EVENT', 'crystalline I-EVENT', 'silica I-EVENT', 'dust I-EVENT', '( O', 'rock O', 'dust O', ') O', '. O']\n",
      "['Pneumatic B-EVENT', 'drilling I-EVENT', 'in I-EVENT', 'mines I-EVENT', 'and O', 'less O', 'commonly O', ', O', 'mining O', 'using O', 'explosives O', ', O', 'would O', 'raise O', 'fine B-EVENT', '- I-EVENT', 'ultra I-EVENT', 'fine I-EVENT', 'crystalline I-EVENT', 'silica I-EVENT', 'dust I-EVENT', '( O', 'rock O', 'dust O', ') O', '. O']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[2])\n",
    "#print(len(sentences[2]))\n",
    "original_sentence = sentences[2]\n",
    "modified_sentences = [None] * len(sentences[2])\n",
    "for i in range(len(sentences[2])):\n",
    "    if i not in range(int(splitSentenceList[0]), int(splitSentenceList[1])) and i not in range(int(splitSentenceList[2]), int(splitSentenceList[3])):\n",
    "        w, t = sentences[2][i].split(' ')\n",
    "        t = 'O'\n",
    "        modified_sentences[i] = ' '.join((w,t))\n",
    "    else:\n",
    "        modified_sentences[i] = sentences[2][i]\n",
    "\n",
    "print(sentences[2])\n",
    "#modified_sentences.append(sentences[2])\n",
    "print(modified_sentences)\n",
    "sentences[2] = original_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emtpydict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m emptydict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m----> 2\u001b[0m \u001b[43memtpydict\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m original_sentence\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(emptydict)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'emtpydict' is not defined"
     ]
    }
   ],
   "source": [
    "emptydict = {'a': 'test'}\n",
    "emtpydict['0'] = original_sentence\n",
    "print(emptydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original_sentence)\n",
    "print(modified_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(labels[467]))\n",
    "#print(len(sentences[0]))\n",
    "##helper = np.zeros((len(sentences[0]), len(sentences[0])))\n",
    "#helper = np.zeros(len(sentences[0]))\n",
    "#print(helper)\n",
    "\n",
    "#for entry in labels[0][0]:\n",
    "#    print(entry)\n",
    "#    for numbers in entry:\n",
    "#        print(numbers)\n",
    "#        helper[numbers-1] = 1\n",
    "#print(helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jcount = 0\n",
    "#for j in helper:\n",
    "#    jcount += 1\n",
    "#    if j > 0:\n",
    "#        print(jcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(labels[1][0][0][0])\n",
    "#print(labels[1][0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(labels[2])\n",
    "#doc = parse_sentence(sentences[2])\n",
    "#doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new df with 1 line per sentence \n",
    "sentence_number=[]\n",
    "wordlist = []\n",
    "taglist = []\n",
    "POSlist = []\n",
    "DEPlist = []\n",
    "customclass = []\n",
    "\n",
    "biggerlist = []\n",
    "biggertaglist = []\n",
    "biggerPOSlist = []\n",
    "biggerDEPlist = []\n",
    "for i in range(len(sentences)):\n",
    "    if labels[i][0][0][0] < labels[0][0][1][0]:\n",
    "        customclass.append('0')\n",
    "    elif labels[i][0][0][0] >= labels[0][0][1][0]:\n",
    "        customclass.append('1')\n",
    "for i in range(len(sentences)):\n",
    "    sentence_number.append(str(i))\n",
    "    \n",
    "    for word in sentences[i]:\n",
    "        wordlist.append(str(word.split()[0]))\n",
    "        taglist.append(str(word.split()[1]))\n",
    "\n",
    "    doc = parse_sentence(sentences[i])\n",
    "    for token in doc:\n",
    "        POSlist.append(token.pos_)\n",
    "        DEPlist.append(token.dep_)\n",
    "            \n",
    "\n",
    "    biggerlist.append(wordlist)\n",
    "    biggertaglist.append(taglist)\n",
    "    biggerPOSlist.append(POSlist)\n",
    "    biggerDEPlist.append(DEPlist)\n",
    "    \n",
    "    wordlist = []\n",
    "    taglist = []\n",
    "    POSlist = []\n",
    "    DEPlist = []\n",
    "\n",
    "train_data = {'Sentence #': sentence_number, 'Wordlist': biggerlist, 'Taglist': biggertaglist, 'POS': biggerPOSlist, 'DEP': biggerDEPlist, 'customclass': customclass,}\n",
    "#train_data = {'Sentence #': sentence_number, 'Wordlist': str(biggerlist), 'Taglist': str(biggertaglist), 'POS': str(biggerPOSlist), 'DEP': str(biggerDEPlist), 'customclass': customclass,}\n",
    "df = pd.DataFrame(data=train_data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n",
    "\n",
    "df = df.fillna(method='ffill')\n",
    "#df['Sentence #'].nunique(), df.Wordlist.nunique(), df.Taglist.nunique(), df.customclass.nunique()\n",
    "#print(df['Sentence #'].nunique())\n",
    "#print(df.Wordlist.nunique())\n",
    "#print(df.Taglist.nunique())\n",
    "#print(df.CustomClass.nunique())\n",
    "\n",
    "#df.groupby('Tag').size().reset_index(name='counts')\n",
    "#print(df.groupby('Tag').size().reset_index(name='counts'))\n",
    "\n",
    "df.groupby('customclass').size().reset_index(name='counts')\n",
    "print(df.groupby('customclass').size().reset_index(name='counts'))\n",
    "\n",
    "#X = df.drop('Tag', axis=1)\n",
    "X = df.drop('customclass', axis=1)\n",
    "#print(X)\n",
    "\n",
    "v = DictVectorizer(sparse=False)\n",
    "\n",
    "X = v.fit_transform(X.to_dict('records'))\n",
    "\n",
    "for i in range(10):\n",
    "    print(X[i])\n",
    "    \n",
    "#y = df.Tag.values\n",
    "y = df.customclass.values\n",
    "classes = np.unique(y)\n",
    "classes = classes.tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per = Perceptron(verbose=10, n_jobs=-1, max_iter=5)\n",
    "per.partial_fit(X_train, y_train, classes)\n",
    "#per.fit(X_train, y_train, classes, coef_init = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#per.partial_fit(X_train, y_train, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_classes = classes.copy()\n",
    "print(new_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_pred=per.predict(X_test), y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "print(per.predict(X_test))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_active_passive(doc):\n",
    "    # https://stackoverflow.com/questions/74528441/detect-passive-or-active-sentence-from-text\n",
    "    passive_rules = [\n",
    "        [\n",
    "            {\"DEP\": \"nsubjpass\"},\n",
    "            {\"DEP\": \"aux\", \"OP\": \"*\"},\n",
    "            {\"DEP\": \"auxpass\"},\n",
    "            {\"TAG\": \"VBN\"},\n",
    "        ],\n",
    "        [\n",
    "            {\"DEP\": \"nsubjpass\"},\n",
    "            {\"DEP\": \"aux\", \"OP\": \"*\"},\n",
    "            {\"DEP\": \"auxpass\"},\n",
    "            {\"TAG\": \"VBZ\"},\n",
    "        ],\n",
    "        [\n",
    "            {\"DEP\": \"nsubjpass\"},\n",
    "            {\"DEP\": \"aux\", \"OP\": \"*\"},\n",
    "            {\"DEP\": \"auxpass\"},\n",
    "            {\"TAG\": \"RB\"},\n",
    "            {\"TAG\": \"VBN\"},\n",
    "        ],\n",
    "    ]\n",
    "    # Create pattern to match active voice use\n",
    "    active_rules = [\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"VBD\", \"DEP\": \"ROOT\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"VBP\"}, {\"TAG\": \"VBG\", \"OP\": \"!\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"DEP\": \"aux\", \"OP\": \"*\"}, {\"TAG\": \"VB\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"DEP\": \"aux\", \"OP\": \"*\"}, {\"TAG\": \"VBG\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"RB\", \"OP\": \"*\"}, {\"TAG\": \"VBG\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"RB\", \"OP\": \"*\"}, {\"TAG\": \"VBZ\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"RB\", \"OP\": \"+\"}, {\"TAG\": \"VBD\"}],\n",
    "    ]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Passive\", passive_rules)\n",
    "    matcher.add(\"Active\", active_rules)\n",
    "    matches = matcher(doc)\n",
    "    matches = [\n",
    "        (nlp.vocab.strings[match_id], doc[start:end])\n",
    "        for match_id, start, end in matches\n",
    "    ]\n",
    "    return matches\n",
    "\n",
    "\n",
    "def predict(sentence):\n",
    "    doc = parse_sentence(sentence)\n",
    "    matches = extract_active_passive(doc)\n",
    "    predictions = []\n",
    "    for ent_1, ent_2 in combinations(doc.ents, 2):\n",
    "        for match_type, match_span in matches:\n",
    "            if SpanGroup(doc, spans=[ent_1, ent_2, match_span]).has_overlap:\n",
    "                match_active = match_type == \"Active\"\n",
    "                if match_active:\n",
    "                    predictions.append(\n",
    "                        ((ent_1.start, ent_1.end), (ent_2.start, ent_2.end))\n",
    "                    )\n",
    "                    break\n",
    "                else:\n",
    "                    predictions.append(\n",
    "                        ((ent_2.start, ent_2.end), (ent_1.start, ent_1.end))\n",
    "                    )\n",
    "                    break\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = parse_sentence(sentences[0])\n",
    "matches = extract_active_passive(doc)\n",
    "print(doc)\n",
    "for match_type, match_span in matches:\n",
    "    print(\"\\t{}: {}\".format(match_type, match_span.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5\n",
    "doc = parse_sentence(sentences[idx])\n",
    "pred = predict(sentences[idx])\n",
    "print(doc)\n",
    "print(\"Ground truth:\")\n",
    "for cause, effect in labels[idx]:\n",
    "    print(\"\\t{} -> {}\".format(doc[cause[0]:cause[1]], doc[effect[0]:effect[1]]))\n",
    "print(\"Predictions:\")\n",
    "for cause, effect in pred:\n",
    "    print(\"\\t{} -> {}\".format(doc[cause[0]:cause[1]], doc[effect[0]:effect[1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "#for sentence in tqdm.tqdm(sentences):\n",
    "for sentence in tqdm(sentences):\n",
    "    predictions.append(predict(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions, references, micro_avg=True):\n",
    "    tp = []\n",
    "    fp = []\n",
    "    fn = []\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        tp.append(len(set(prediction) & set(reference)))\n",
    "        fp.append(len(set(prediction) - set(reference)))\n",
    "        fn.append(len(set(reference) - set(prediction)))\n",
    "    if micro_avg:\n",
    "        tp = [sum(tp)]\n",
    "        fp = [sum(fp)]\n",
    "        fn = [sum(fn)]\n",
    "    precision = [0 if tp[i] == 0 else tp[i] / (tp[i] + fp[i]) for i in range(len(tp))]\n",
    "    recall = [0 if tp[i] == 0 else tp[i] / (tp[i] + fn[i]) for i in range(len(tp))]\n",
    "    f1 = [\n",
    "        0\n",
    "        if precision[i] * recall[i] == 0\n",
    "        else 2 * precision[i] * recall[i] / (precision[i] + recall[i])\n",
    "        for i in range(len(tp))\n",
    "    ]\n",
    "    precision = sum(precision) / len(precision)\n",
    "    recall = sum(recall) / len(recall)\n",
    "    f1 = sum(f1) / len(f1)\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "micro_precision, micro_recall, micro_f1 = evaluate(predictions, labels, True)\n",
    "macro_precision, macro_recall, macro_f1 = evaluate(predictions, labels, False)\n",
    "\n",
    "print(\"Micro Precision: {:.2f}\".format(micro_precision))\n",
    "print(\"Micro Recall: {:.2f}\".format(micro_recall))\n",
    "print(\"Micro F1: {:.2f}\".format(micro_f1))\n",
    "print(\"Macro Precision: {:.2f}\".format(macro_precision))\n",
    "print(\"Macro Recall: {:.2f}\".format(macro_recall))\n",
    "print(\"Macro F1: {:.2f}\".format(macro_f1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
