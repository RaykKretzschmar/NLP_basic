{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, SpanGroup\n",
    "from spacy.matcher import Matcher\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "from tqdm import autonotebook as tqdm\n",
    "from spacy.training import biluo_tags_to_spans\n",
    "import iobes\n",
    "import re\n",
    "from itertools import combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"./data/teaching-dataset\")\n",
    "with (data_dir / \"relation_classification_text_train.zip\").open(\"rb\") as file:\n",
    "    zip_file = ZipFile(file)\n",
    "    with zip_file.open(\"input.txt\") as f:\n",
    "        sentences = [\n",
    "            sentence.split(\"\\n\") for sentence in f.read().decode(\"utf-8\").split(\"\\n\\n\")\n",
    "        ]\n",
    "with (data_dir / \"relation_classification_references_train.zip\").open(\"rb\") as file:\n",
    "    zip_file = ZipFile(file)\n",
    "    with zip_file.open(\"references.txt\") as f:\n",
    "        labels = []\n",
    "        for line in f.read().decode(\"utf-8\").split(\"\\n\"):\n",
    "            relations = []\n",
    "            for relation in re.finditer(r\"\\(\\((\\d+),(\\d+)\\),\\((\\d+),(\\d+)\\)\\)\", line):\n",
    "                relation = (\n",
    "                    (int(relation.group(1)), int(relation.group(2))),\n",
    "                    (int(relation.group(3)), int(relation.group(4))),\n",
    "                )\n",
    "                relations.append(relation)\n",
    "            labels.append(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence(sentence):\n",
    "    words = []\n",
    "    tags = []\n",
    "    for item in sentence:\n",
    "        word, tag = item.split(\" \")\n",
    "        words.append(word)\n",
    "        tags.append(tag)\n",
    "    doc = Doc(nlp.vocab, words=words)\n",
    "    doc = nlp(doc)\n",
    "    tags = iobes.bio_to_bilou(tags)\n",
    "    doc.ents = biluo_tags_to_spans(doc, tags)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in sentences[0]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = parse_sentence(sentences[0])\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_active_passive(doc):\n",
    "    # https://stackoverflow.com/questions/74528441/detect-passive-or-active-sentence-from-text\n",
    "    passive_rules = [\n",
    "        [\n",
    "            {\"DEP\": \"nsubjpass\"},\n",
    "            {\"DEP\": \"aux\", \"OP\": \"*\"},\n",
    "            {\"DEP\": \"auxpass\"},\n",
    "            {\"TAG\": \"VBN\"},\n",
    "        ],\n",
    "        [\n",
    "            {\"DEP\": \"nsubjpass\"},\n",
    "            {\"DEP\": \"aux\", \"OP\": \"*\"},\n",
    "            {\"DEP\": \"auxpass\"},\n",
    "            {\"TAG\": \"VBZ\"},\n",
    "        ],\n",
    "        [\n",
    "            {\"DEP\": \"nsubjpass\"},\n",
    "            {\"DEP\": \"aux\", \"OP\": \"*\"},\n",
    "            {\"DEP\": \"auxpass\"},\n",
    "            {\"TAG\": \"RB\"},\n",
    "            {\"TAG\": \"VBN\"},\n",
    "        ],\n",
    "    ]\n",
    "    # Create pattern to match active voice use\n",
    "    active_rules = [\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"VBD\", \"DEP\": \"ROOT\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"VBP\"}, {\"TAG\": \"VBG\", \"OP\": \"!\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"DEP\": \"aux\", \"OP\": \"*\"}, {\"TAG\": \"VB\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"DEP\": \"aux\", \"OP\": \"*\"}, {\"TAG\": \"VBG\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"RB\", \"OP\": \"*\"}, {\"TAG\": \"VBG\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"RB\", \"OP\": \"*\"}, {\"TAG\": \"VBZ\"}],\n",
    "        [{\"DEP\": \"nsubj\"}, {\"TAG\": \"RB\", \"OP\": \"+\"}, {\"TAG\": \"VBD\"}],\n",
    "    ]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Passive\", passive_rules)\n",
    "    matcher.add(\"Active\", active_rules)\n",
    "    matches = matcher(doc)\n",
    "    matches = [\n",
    "        (nlp.vocab.strings[match_id], doc[start:end])\n",
    "        for match_id, start, end in matches\n",
    "    ]\n",
    "    return matches\n",
    "\n",
    "\n",
    "def predict(sentence):\n",
    "    doc = parse_sentence(sentence)\n",
    "    matches = extract_active_passive(doc)\n",
    "    predictions = []\n",
    "    for ent_1, ent_2 in combinations(doc.ents, 2):\n",
    "        for match_type, match_span in matches:\n",
    "            if SpanGroup(doc, spans=[ent_1, ent_2, match_span]).has_overlap:\n",
    "                match_active = match_type == \"Active\"\n",
    "                if match_active:\n",
    "                    predictions.append(\n",
    "                        ((ent_1.start, ent_1.end), (ent_2.start, ent_2.end))\n",
    "                    )\n",
    "                    break\n",
    "                else:\n",
    "                    predictions.append(\n",
    "                        ((ent_2.start, ent_2.end), (ent_1.start, ent_1.end))\n",
    "                    )\n",
    "                    break\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = parse_sentence(sentences[0])\n",
    "matches = extract_active_passive(doc)\n",
    "print(doc)\n",
    "for match_type, match_span in matches:\n",
    "    print(\"\\t{}: {}\".format(match_type, match_span.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5\n",
    "doc = parse_sentence(sentences[idx])\n",
    "pred = predict(sentences[idx])\n",
    "print(doc)\n",
    "print(\"Ground truth:\")\n",
    "for cause, effect in labels[idx]:\n",
    "    print(\"\\t{} -> {}\".format(doc[cause[0]:cause[1]], doc[effect[0]:effect[1]]))\n",
    "print(\"Predictions:\")\n",
    "for cause, effect in pred:\n",
    "    print(\"\\t{} -> {}\".format(doc[cause[0]:cause[1]], doc[effect[0]:effect[1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for sentence in tqdm.tqdm(sentences):\n",
    "    predictions.append(predict(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions, references, micro_avg=True):\n",
    "    tp = []\n",
    "    fp = []\n",
    "    fn = []\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        tp.append(len(set(prediction) & set(reference)))\n",
    "        fp.append(len(set(prediction) - set(reference)))\n",
    "        fn.append(len(set(reference) - set(prediction)))\n",
    "    if micro_avg:\n",
    "        tp = [sum(tp)]\n",
    "        fp = [sum(fp)]\n",
    "        fn = [sum(fn)]\n",
    "    precision = [0 if tp[i] == 0 else tp[i] / (tp[i] + fp[i]) for i in range(len(tp))]\n",
    "    recall = [0 if tp[i] == 0 else tp[i] / (tp[i] + fn[i]) for i in range(len(tp))]\n",
    "    f1 = [\n",
    "        0\n",
    "        if precision[i] * recall[i] == 0\n",
    "        else 2 * precision[i] * recall[i] / (precision[i] + recall[i])\n",
    "        for i in range(len(tp))\n",
    "    ]\n",
    "    precision = sum(precision) / len(precision)\n",
    "    recall = sum(recall) / len(recall)\n",
    "    f1 = sum(f1) / len(f1)\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "micro_precision, micro_recall, micro_f1 = evaluate(predictions, labels, True)\n",
    "macro_precision, macro_recall, macro_f1 = evaluate(predictions, labels, False)\n",
    "\n",
    "print(\"Micro Precision: {:.2f}\".format(micro_precision))\n",
    "print(\"Micro Recall: {:.2f}\".format(micro_recall))\n",
    "print(\"Micro F1: {:.2f}\".format(micro_f1))\n",
    "print(\"Macro Precision: {:.2f}\".format(macro_precision))\n",
    "print(\"Macro Recall: {:.2f}\".format(macro_recall))\n",
    "print(\"Macro F1: {:.2f}\".format(macro_f1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
